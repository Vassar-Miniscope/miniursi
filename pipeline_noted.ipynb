{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTED PIPELINE FROM DENISE CAI LAB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the annotated pipeline provided in the minian Github. While it is not completely functional (it has bugs they are actively fixing) it does include all of the relevant information about the pipeline and can be used in tandem with the pipeline_calibration.ipynb notebook for further contextualization of the steps and troubleshooting. Other issues may be found in the 'Issues' section of their Github. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "notes"
    ]
   },
   "source": [
    "# About this document\n",
    "\n",
    "The purpose of this annotated version of the minian pipeline is to guide the user through each step of the code, working with a short demo movie. The intention is to enable the user to understand the code as much as possible so that they are equipped with the knowledge necessary to customize the code for their own needs, regardless of prior programming skills. This version is **not** supposed to be run as a production tool for analyzing your own videos. If you click \"Run All\" on this document, you **will** likely encounter errors. For the purposes of your own analyses, consider modifying the accompanying **pipeline.ipynb** and **batch_processing.ipynb** to suit your needs.\n",
    "\n",
    "Before we start, it s highly recommended that you get familiar with basic python concepts and operations like [string manipulation](https://docs.python.org/3.4/library/string.html), [tuples, lists and dictionaries](https://docs.python.org/3/tutorial/datastructures.html), as well as a little bit about [object-oriented programming](https://python.swaroopch.com/oop.html) and [python modules](https://docs.python.org/3/tutorial/modules.html).\n",
    "\n",
    "Another note on the styling of this document: most of the sentences should hopefully make sense if taken literally. However, some special formatting of the text is used to demonstrate the close relationship between the concepts discussed and the code, as well as encouraging the reader to understand the Python syntax. Specifically:\n",
    "\n",
    "-  a [hyperlink](https://en.wikipedia.org/wiki/Hyperlink) usually points to a well-defined python module, class or methods, especially when that concept is first encountered in this document. The link usually points to the official documentation of that concept, which in some cases might not be the best place to start for a beginner. If you find the documentation puzzling, try to google the concept in question and find a tutorial that best suits you.\n",
    "-  an inline `code` usually refers to a name that already exsists in the [namespace](https://docs.python.org/3/tutorial/classes.html#python-scopes-and-namespaces) (i.e. the context where we run the codes in this document). It can be a previously encountered concept, but more often it referes to variable names or method names that we [imported](https://docs.python.org/3/reference/import.html) or have defined along the way.\n",
    "-  **bold** texts are used more loosely to highlight anything that requires more attention. Though they are not used as carefully as previous formats, they often refer to specific values that a variable or method arguments can assume.\n",
    "-  <div class=\"alert alert-info\">\n",
    "    Blue tip boxes are used to provide direct instructions and coding tips to help users run through this pipeline smoothly! :) \n",
    "</div>\n",
    "- <div class=\"alert alert-success\" role=\"alert\">\n",
    "    Green tip boxes are used to let the user know what to expect from the output visualization result of parameter exploring.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "notes"
    ]
   },
   "source": [
    "# Workflow\n",
    "\n",
    "As shown in the workflow below, there are 7 main sections in this pipeline. Results will be saved along the way, following **Motion correction**, **Background removal**, **Initialization**, and **CNMF**. Therefore, when you run through the pipeline, if you decide to restart/shutdown the kernal before you finish, you don't have to re-run everything  the next time you want to pick this up (with the exception of the **Setting Up** module). For example, if you restart/shutdown the kernal after **Initialization**, when you return to your data you can simply rerun the **Setting Up** module, and then go directly to **CNMF**.\n",
    "\n",
    "Before we dive into the pipeline, we will also introduce the most powerful and important aspect of this pipeline -- parameter exploring.  **CNMF** has been daunting to some because of the many parameters involved in its implementation. The purpose of minian's interactive visualization steps is to make the impact of all parameters transparent to users by allowing them to view the direct impact of parameter manipulation on the data, helping them to find the parameters best suited to their data. We will go into greater detail on how to use and interpret these viusalization steps later on, but for now, just know that if you are daunted by CNMF, these steps will likely be the most helpful.\n",
    "\n",
    "![workflow](img/Workflow_v2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells under this section should be executed every time the kernel is restarted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell loads the necessary modules for minian to run and usually should not be modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMBA_NUM_THREADS\"] = \"1\"\n",
    "import gc\n",
    "import psutil\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import holoviews as hv\n",
    "import matplotlib.pyplot as plt\n",
    "import bokeh.plotting as bpl\n",
    "import dask.array as da\n",
    "import pandas as pd\n",
    "import dask\n",
    "import datashader as ds\n",
    "import itertools as itt\n",
    "import papermill as pm\n",
    "import ast\n",
    "import functools as fct\n",
    "from holoviews.operation.datashader import datashade, regrid, dynspread\n",
    "from datashader.colors import Sets1to3\n",
    "from dask.diagnostics import ProgressBar\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set path and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we set all of the parameters that control how the notebook will behave. Ideally, the following cell would be the only thing you have to change when analyzing different datasets. Indeed, we put all parameters here under a single cell to facilitate batch processing. Most of the parameters will not make sense until we reach and discuss the corresponding steps in the pipeline. For this reason, here we discuss only some initial parameters that have a broader impact, and leave the discussion of specific parameters for later.\n",
    "\n",
    "* `minian_path` should be the path that contains a folder named **\"minian\"** , under which the actual code of **minian** (.py files) reside. The default value **\\.** means \"current folder\", which should work in most cases, unless you want to try out another version of minian that is not in the same folder as this notebook.\n",
    "\n",
    "* `dpath` is the folder that contains the actual videos to be processed, which are usually named **\"msCam\\*.avi\"** where \\* is a number.\n",
    "\n",
    "* `interactive` controls whether interactive plots will be shown.  Note that interactive plotting requires computation to be carried out, and thus could be very inefficient when the data are not in the memory (in particular, those steps where video is played).  However, if your video is large, you may want to set `interactive = True` for the purposes of paramater exploration, but have `in_memory` set to false.   \n",
    "<div class=\"alert alert-info\">\n",
    "In practice, when you want to visualize interactive plots while figuring out the best parameters for your data, you would want in_memory=True and interactive=True, as long as your data can fit in the memory. On the other hand, once you finalize your parameters and are ready for batch processing, you want both of them to be set as False.\n",
    "</div>\n",
    "\n",
    "* `output_size` controls the relative size of all the plots on a scale of 0-100 percent, though it can be set to values >100 without any problem. Adjust this to please your eye. \n",
    "\n",
    "* `param_save_minian` specifies how data is to be saved. `dpath` is the folder path defining where you want the data to be saved. We recommand using the same dpath as where you load the imaging data from. `fname` is the name of your dataset. In `backend`, `'zarr'` is designed for parallel and out-of-core computation, and is the current default for minian. That said, its support is experimental for now and does not support incremental writing (i.e. it will be a pain to update part of an exsisting dataset). `meta_dict` is a `dictionary` that is used to construct meta data for the final labeled data structure.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "The defult meta_dict is assumes data is stored in heirarchiically arranged folders, as shown below. We <strong> recommand </strong> users to structure their data like this so that they don't have to adjust this meta_dict setting.  This is also the default manner in which Miniscope data is saved. However, if you already have a preferred way to store your data, you can simply change the value of meta_dict in this parameter to suit your needs!\n",
    "</div>\n",
    "\n",
    "**recommended folder structure**\n",
    "\n",
    "![Folder Structure](img/folder_structure.png)\n",
    "\n",
    "The default value can be read as follows: the name of the last folder (`values`=-1) in `dpath` (the folder that directly contains the videos) will be used to designate the value of a field named **'session_id'**. The name of the second-to-last folder (`values`=-2) in `dpath` will be used to designate the value for **'session'** and so on. Both the `keys` (field names) and `values` (numbers indicating which level of folder name should be used) of `meta_dict` can be modified to suit your data structure. `overwrite` is a boolean value controlling whether the data is overwritten if a file already exsists. We set it to `True` here so you can easily play with the demo multiple times, but **use extreme caution** with this during actual analysis -- in addition to erasing prior data that may be important to you, under certain circumstances it is possible for existing file structures to cause compatibablity issues and data will be saved improperly.  If you want to re-analyze a video from scratch using different parameters, it is recommended that you delete existing data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#Set up Initial Basic Parameters#\n",
    "minian_path = \".\"\n",
    "dpath = \"./demo_movies\"\n",
    "subset = dict(frame=slice(0,None))\n",
    "subset_mc = None\n",
    "interactive = True\n",
    "output_size = 100\n",
    "param_save_minian = {\n",
    "    'dpath': dpath,\n",
    "    'fname': 'minian',\n",
    "    'backend': 'zarr',\n",
    "    'meta_dict': dict(session_id=-1, session=-2, animal=-3),\n",
    "    'overwrite': True}\n",
    "\n",
    "#Pre-processing Parameters#\n",
    "param_load_videos = {\n",
    "    'pattern': 'msCam[0-9]+\\.avi$',\n",
    "    'dtype': np.uint8,\n",
    "    'downsample': dict(frame=2),\n",
    "    'downsample_strategy': 'subset'}\n",
    "param_denoise = {\n",
    "    'method': 'median',\n",
    "    'ksize': 5}\n",
    "param_background_removal = {\n",
    "    'method': 'tophat',\n",
    "    'wnd': 10}\n",
    "\n",
    "#Motion Correction Parameters#\n",
    "subset_mc = None\n",
    "param_estimate_shift = {\n",
    "    'dim': 'frame',\n",
    "    'max_sh': 20}\n",
    "\n",
    "#Initialization Parameters#\n",
    "param_seeds_init = {\n",
    "    'wnd_size': 2000,\n",
    "    'method': 'rolling',\n",
    "    'stp_size': 1000,\n",
    "    'nchunk': 100,\n",
    "    'max_wnd': 15,\n",
    "    'diff_thres': 3}\n",
    "param_pnr_refine = {\n",
    "    'noise_freq': 0.06,\n",
    "    'thres': 1,\n",
    "    'med_wnd': None}\n",
    "param_ks_refine = {\n",
    "    'sig': 0.01}\n",
    "param_seeds_merge = {\n",
    "    'thres_dist': 20,\n",
    "    'thres_corr': 0.7,\n",
    "    'noise_freq': 0.25}\n",
    "param_initialize = {\n",
    "    'thres_corr': 0.8,\n",
    "    'wnd': 25,\n",
    "    'noise_freq': 0.25}\n",
    "\n",
    "#CNMF Parameters#\n",
    "param_get_noise = {\n",
    "    'noise_range': (0.1, 0.5),\n",
    "    'noise_method': 'logmexp'}\n",
    "param_first_spatial = {\n",
    "    'dl_wnd': 10,\n",
    "    'sparse_penal': 0.01,\n",
    "    'update_background': True,\n",
    "    'normalize': True,\n",
    "    'zero_thres': 'eps'}\n",
    "param_first_temporal = {\n",
    "    'noise_freq': 0.06,\n",
    "    'sparse_penal': 0.1,\n",
    "    'p': 1,\n",
    "    'add_lag': 20,\n",
    "    'use_spatial': False,\n",
    "    'jac_thres': 0.2,\n",
    "    'zero_thres': 1e-8,\n",
    "    'max_iters': 200,\n",
    "    'use_smooth': True,\n",
    "    'scs_fallback': False,\n",
    "    'post_scal': True}\n",
    "param_first_merge = {\n",
    "    'thres_corr': 0.8}\n",
    "param_second_spatial = {\n",
    "    'dl_wnd': 10,\n",
    "    'sparse_penal': 0.0005,\n",
    "    'update_background': True,\n",
    "    'normalize': True,\n",
    "    'zero_thres': 'eps'}\n",
    "param_second_temporal = {\n",
    "    'noise_freq': 0.06,\n",
    "    'sparse_penal': 0.05,\n",
    "    'p': 1,\n",
    "    'add_lag': 20,\n",
    "    'use_spatial': False,\n",
    "    'jac_thres': 0.2,\n",
    "    'zero_thres': 1e-8,\n",
    "    'max_iters': 500,\n",
    "    'use_smooth': True,\n",
    "    'scs_fallback': False,\n",
    "    'post_scal': True}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import minian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell loads **minian** and usually should not be modified. If you encounter an `ImportError`, check that you followed the installation instructions and that `minian_path` is pointing to the right place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "sys.path.append(minian_path)\n",
    "from minian.utilities import load_params, load_videos, scale_varr, scale_varr_da, save_variable, open_minian, save_minian, handle_crash, get_optimal_chk, rechunk_like\n",
    "from minian.preprocessing import remove_brightspot, gradient_norm, denoise, remove_background, stripe_correction\n",
    "from minian.motion_correction import estimate_shifts, apply_shifts\n",
    "from minian.initialization import seeds_init, gmm_refine, pnr_refine, intensity_refine, ks_refine, seeds_merge, initialize\n",
    "from minian.cnmf import get_noise_fft, update_spatial, compute_trace, update_temporal, unit_merge, smooth_sig\n",
    "from minian.visualization import VArrayViewer, CNMFViewer, generate_videos, visualize_preprocess, visualize_seeds, visualize_gmm_fit, visualize_spatial_update, visualize_temporal_update, roi_draw, write_video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## module initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell handles initialization of modules and parameters necessary for minian to be run and usually should not be modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dpath = os.path.abspath(dpath)\n",
    "if interactive:\n",
    "    hv.notebook_extension('bokeh')\n",
    "    pbar = ProgressBar(minimum=2)\n",
    "    pbar.register()\n",
    "else:\n",
    "    hv.notebook_extension('matplotlib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the pre-processing steps that follow, videos will be loaded and any initial processing (downsampling, subsetting, denoising) will be performed.\n",
    "\n",
    "All functions are evaluated lazily, which means that initially only a \"plan\" for the actual computation will be created, without its execution. Actual computations are carried out only when results are being saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading videos and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the values of `param_load_videos`:\n",
    "\n",
    "```python\n",
    "param_load_videos = {\n",
    "    'pattern': 'msCam[0-9]+\\.avi$',\n",
    "    'dtype': np.uint8,\n",
    "    'downsample': dict(frame=2),\n",
    "    'downsample_strategy': 'subset'}\n",
    "```\n",
    "\n",
    "The first argument of `load_videos` should be the path that contains the videos, which is the file folder we previously defined (`dpath`) we already defined. We then pass the dictionary, `param_load_videos`, defined earlier, which specifies several relevant arguments. The argument `pattern` is optional and is the [regular expression](https://docs.python.org/3/library/re.html) used to filter files under the specified folder. The default value **'msCam[0-9]+\\.avi$'** means that a file can only be loaded if its filename contains **'msCam'**, followed by at least one number, then **'.avi'** as the end of the filename. This can be changed to suit the naming convention of your videos. `dtype` is the underlying [data type](https://docs.scipy.org/doc/numpy-1.15.0/user/basics.types.html) of the data. Usually `uint8` is good and should be preferred to save memory demand. The resulting \"video array\" `varr` contains three dimensions: `height`, `width`, and `frame`. If you wish to downsample the video, pass in a `dictionary` to `downsample`, whose keys should be the name of dimensions and values an integer specifying how many times that dimension should be reduced. For example, `downsample=dict('frame'=2)` will temporally downsample the video with a factor of 2. Instead, if you do not wish to downsample your data, simply pass in `downsample=None`. `downsample_strategy` will assume two values: either `'subset'`, meaning downsampling are carried out simply by subsetting the data, or `'mean'`, meaning a mean will be calculated on the window of downsampling (the latter being slower).\n",
    "\n",
    "`param_load_videos['downsample']` should be specified as a python [dictionary](https://docs.python.org/3/tutorial/datastructures.html#dictionaries), whose `keys` are the dimensions along which subsetting should be done, and whose `values` specify how subsetting should be done. \n",
    "<div class=\"alert alert-info\">     \n",
    "This is a good opportunity to introduce how to manipulate parameters while you are running through this pipeline. If you want to modify the parameters, you can either go back to the initial parameter setting cell and change things there. Alternatively, you can add a cell of code in which you change one or more paramaters. For example, if you want to change the downsampling setting for your data, you can: \n",
    "</div>                                                            \n",
    "\n",
    "**Option 1--Go back to initial parameter setting code cell, change the parameters setting there, then rerun the parameter setting cell:**\n",
    "\n",
    "\n",
    "**Example 1: Stop downsampling**\n",
    "                                                               \n",
    "```python\n",
    "param_load_videos = {\n",
    "    'pattern': 'msCam[0-9]+\\.avi$',\n",
    "    'dtype': np.float32,\n",
    "    'in_memory': in_memory,\n",
    "    'downsample': dict(frame=2),\n",
    "    'downsample_strategy': 'subset'}\n",
    "```\n",
    "\n",
    "**change this to:**\n",
    "\n",
    "```python\n",
    "param_load_videos = {\n",
    "    'pattern': 'msCam[0-9]+\\.avi$',\n",
    "    'dtype': np.float32,\n",
    "    'in_memory': in_memory,\n",
    "    'downsample': None,\n",
    "    'downsample_strategy': 'subset'}\n",
    "```\n",
    "\n",
    "**Example 2: Changing the downsampling setting from by `frame` to by `height` and `width`, and also changing the downsampling strategy to `mean`.**\n",
    "\n",
    "```python\n",
    "param_load_videos = {\n",
    "    'pattern': 'msCam[0-9]+\\.avi$',\n",
    "    'dtype': np.float32,\n",
    "    'in_memory': in_memory,\n",
    "    'downsample': dict(frame=2),\n",
    "    'downsample_strategy': 'subset'}\n",
    "```\n",
    "\n",
    "**change this to:**\n",
    "\n",
    "```python\n",
    "param_load_videos = {\n",
    "    'pattern': 'msCam[0-9]+\\.avi$',\n",
    "    'dtype': np.float32,\n",
    "    'in_memory': in_memory,\n",
    "    'downsample': dict(height=2,width=2),\n",
    "    'downsample_strategy': 'mean'}\n",
    "```\n",
    "                                                             \n",
    "**Option 2--Insert a code cell by clicking the little + symbol on the top row of jupyter notebook. Then change the specific 'keys' with the 'value' you want to asign to them as shown below, and run this new code cell.**\n",
    "\n",
    "**Example 1: Stop downsampling**\n",
    "```python                                                           \n",
    "param_load_videos['downsample'] = None\n",
    "``` \n",
    "\n",
    "**Example 2: Changing the downsampling setting from by `frame` to by `height` and `width`, and also changing the downsampling strategy to `mean`.**\n",
    "```python                                                           \n",
    "param_load_videos['downsample'] = dict(height=2,width=2)\n",
    "param_load_videos['strategy'] = 'mean'\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "varr = load_videos(dpath, **param_load_videos)\n",
    "chk = get_optimal_chk(varr.astype(float), dim_grp=[('frame',), ('height', 'width')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous code cell loaded the videos and concatenated them together into the unitary data object `varr`, which is a [xarray.DataArray](http://xarray.pydata.org/en/stable/generated/xarray.DataArray.html#xarray.DataArray). Now is a perfect time to familiarize yourself with this data structure and the [xarray](https://xarray.pydata.org/en/stable/) module in general, since we will be using these data structures throughout the analysis. Basically, a `xarray.DataArray` is a labeled N-dimensional array, with many useful properties that make them easy to manipulate. We can ask the computer to print out some information of `varr` by calling its name (as with any other variable):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualize raw data and optionally set roi for motion correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see now that `varr` is a `xarray.DataArray` with a [name](https://xarray.pydata.org/en/stable/generated/xarray.DataArray.name.html#xarray.DataArray.name) `'demo_movies'` and three dimensions: `frame`, `height` and `width`; and each dimension is labeled with ascending natural numbers. The [dtype](https://xarray.pydata.org/en/stable/generated/xarray.DataArray.dtype.html#xarray.DataArray.dtype) ([data type](https://docs.scipy.org/doc/numpy-1.14.0/user/basics.types.html)) of `varr` is `numpy.uint8`\n",
    "\n",
    "In addition to this information, we can visualize `varr` with the help of `VArrayViewer`, which shows the array as a movie. You can also plot summary traces like mean fluorescnece across `frame` by passing in a `list` of names of traces you want. Currently `\"mean\"`, `\"min\"`, `\"max\"` and `\"diff\"` are supported, where `\"diff\"` is mean fluorescent value difference across all pixels in a `frame`.\n",
    "\n",
    "Finally `VArrayViewer` support a box drawing tool where you can draw an arbitrary box in the field of view and record this box as a mask using the \"save mask\" button. The mask is saved as `vaviewer.mask`. This mask could be useful for other steps, for example, when you want to run motion correction on a sub-region of field of view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.output(size=output_size)\n",
    "if interactive:\n",
    "    vaviewer = VArrayViewer(varr, framerate=5, summary=['mean', 'max'])\n",
    "    display(vaviewer.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you decided to set a mask for motion correction, the following cell is an example of how to convert the mask into a `subset_mc` parameter that can be later passed into motion correction functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interactive:\n",
    "    try:\n",
    "        subset_mc = list(vaviewer.mask.values())[0]\n",
    "    except IndexError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## subset part of video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding to pre-processing, it's good practice to check if there is anything obviously wrong with the video (e.g. the camera suddenly dropped, resulting in dark frames). This can usually be observed by visualizing the video and checking the mean fluorescence plot. To take out bad `frame`s, let's say, `frame` after 800, we can utilize the [xarray.DataArray.sel](http://xarray.pydata.org/en/stable/generated/xarray.DataArray.sel.html) method and [slice](https://docs.python.org/3/library/functions.html#slice):\n",
    "<div class=\"alert alert-info\">                                                               \n",
    "Here is good chance to briefly introduce how to use <strong>.sel</strong> and <strong>slice</strong> properly since this will be super useful to handle your imaging video data.  <strong>See below:</strong>\n",
    "</div>   \n",
    "\n",
    "By using DataArray.sel, you can easily use the basic format:\n",
    "```python\n",
    "subsetDataArray = DataArray.sel(dimsA=object(value))\n",
    "```\n",
    "This will assign the data array you selected out to the new data array which is `subsetDataArray` in this case.\n",
    "\n",
    "Using slice() function, you can select a subset or a portion of your data by calling the function like this:\n",
    "```python\n",
    "subsetDimsA = slice(start, stop, step)\n",
    "```\n",
    "You can combine xarray.DataArray.sel method and slice function to manipulate your multidimensional imaging data! Note that slice object is just one of the useful objects that you can use here. \n",
    "\n",
    "**Example 1**: Say that you want to get rid of the frames after 800: \n",
    "\n",
    "```python \n",
    "varr_ref = varr.sel(frame=slice(None, 800))\n",
    "```\n",
    "This will subset `varr` along the `frame` dimension from the begining (`None`) to the `frame` labeled **800**, then assign the result back to `varr_ref`, which is equivalent to taking out `frame` from **801** to the end. Note you can do the same thing to other dimensions like `height` and `width` to take out certain pixels of your video for all `frame`s. For more information on using `xarray.DataArray.sel`, as well as other indexing strategies, see [xarray documentation](http://xarray.pydata.org/en/stable/indexing.html)\n",
    "\n",
    "**Example 2**: If you want to get rid of the timestamp located in the last row of pixels in height dimention for the wireless Miniscope recording, **.isel** will be a useful. See [xarray.Dataset.isel documentation](http://xarray.pydata.org/en/stable/generated/xarray.Dataset.isel.html). Different from **.sel**, **.isel** subsets out data by index, rather than by coordinate: \n",
    "```python\n",
    "varr_ref = varr.isel(height=slice(None, -1))\n",
    "```\n",
    "This will subset `varr` along the `height` dimension from the begining to the second-to-last row of pixels, then assign the result back to `varr_ref`, which is equivalent to taking out last row of `height`. \n",
    "\n",
    "If your `varr` is fine, just assign it to `varr_ref` to keep the naming consitent with later code.\n",
    "\n",
    "**In production mode** -- pipeline.ipynb or batch_processing.ipynb -- we usually use the `subset` parameter defined above under the module paramater selection to control subsetting. Recall the `subset` parameter. \n",
    "```python\n",
    "subset = None\n",
    "```\n",
    " `subset` is used to subset the data. `subset` should be specified as python [dictionary](https://docs.python.org/3/tutorial/datastructures.html#dictionaries), whose `keys` are the dimensions along which subsetting should be done, and whose `values` should specify how subsetting should be done (usually a [slice object](https://docs.python.org/3/c-api/slice.html)). A good usecase for this is to take out some troublesome frames or to take out some bad pixels. Alternatively, perhaps you are only interested in analalyzing the first few minutes of a much longer recording session.\n",
    "<div class=\"alert alert-info\">                                                               \n",
    "Here is good chance to introduce how to create a dictionary, and more importantly, how to use it! <strong>See below:</strong>\n",
    "</div>\n",
    "\n",
    "The basic format of a dictionary takes the following form:\n",
    "```python\n",
    "dictionary = {'key1': value1, 'key2': value2, ... 'keyN': valueN}\n",
    "```\n",
    "                                                           \n",
    "For example, if you only want to keep the first 800 `frames`, and the `height` and `width` from 100 to 200, you could create this subset dictionary:\n",
    "\n",
    "```python\n",
    "subset = {\n",
    "    'frame': slice(0, 800),\n",
    "    'height': slice(100, 200),\n",
    "    'width': slice(100, 200)}\n",
    "```\n",
    "                                                               \n",
    "Similarly, after you have a dictionary, you can call a specific key in this dictionary and change the value accordingly. We can use the same example here, say if you want to get rid of `frames` from 801 to the end:\n",
    "\n",
    "```python\n",
    "subset['frame'] = slice(None, 800)\n",
    "```\n",
    "<div class=\"alert alert-info\">                                                               \n",
    "Leaving subset in default setting will result in no selection and is thus equivalent to assigning varr back to varr_ref.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varr_ref = varr.sel(subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## glow removal and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we remove the general glow background caused by viganetting effect. We simply calculate a minimum projection across all `frame`s and subtract that projection from all `frame`s. A benefit of doing this is you could interpret the result as \"change of fluorescence from baseline\", while preserving the linear scale of the raw data, which is usually the range of a 8-bit integer -- 0-255. The result can be visualized again with `VArrayViewer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "varr_min = varr_ref.min('frame').compute()\n",
    "varr_ref = varr_ref - varr_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.output(size=output_size)\n",
    "if interactive:\n",
    "    vaviewer = VArrayViewer(\n",
    "        [varr.rename('original'), varr_ref.rename('glow_removed')],\n",
    "        framerate=5,\n",
    "        summary=None,\n",
    "        layout=True)\n",
    "    display(vaviewer.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## denoise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step carries out denoising of the video frame by frame, using the `denoise` function. The function `denoise` takes in two required arguments: the first is the video array to be processed (`varr_reff`), and the second, `method`, is a string specifying the denoising method to use. Right now three methods are supported: `'gaussian'`, `'median'` and `'anisotropic'`. Under the hood, `denoise` simply calls another function frame by frame in a parallel fashion. For `method='gaussian'` it calls [GaussianBlur](https://www.docs.opencv.org/3.3.0/d4/d86/group__imgproc__filter.html#gaabe8c836e97159a9193fb0b11ac52cf1) from the `OpenCV` package. For`method='median'` it calls [MedianBlur](https://docs.opencv.org/3.4.3/d4/d86/group__imgproc__filter.html#ga564869aa33e58769b4469101aac458f9) from the `OpenCV` package. For `method='anisotropic'` it calls [anisotropic_diffusion](http://loli.github.io/medpy/generated/medpy.filter.smoothing.anisotropic_diffusion.html) from the `medpy` package. All additional [keyword arguments](https://docs.python.org/3.7/tutorial/controlflow.html#keyword-arguments) passed into `denoise` are directly passed into one of those two denoising functions under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that by default we use a median filter for enoising:\n",
    "\n",
    "```python\n",
    "param_first_denoise = {\n",
    "    'method': 'median',\n",
    "    'ksize': 5}\n",
    "```\n",
    "\n",
    "There is only one parameter controlling how the filtering is done: the kernel size (`ksize`) of the filter. The effect of this parameter can be visualized with the tool below.\n",
    "<div class=\"alert alert-info\">                                                               \n",
    "Generally ksize=5 is good (approximately half the diamater of the largest cell). Note that if you do want to play with the ksize, it has to be odd number.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.output(size=output_size)\n",
    "if interactive:\n",
    "    display(visualize_preprocess(varr_ref.isel(frame=0), denoise, method=['median'], ksize=[5, 7, 9, 11, 13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varr_ref = denoise(varr_ref, **param_denoise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## backgroun removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the parameters for background removal:\n",
    "\n",
    "```python\n",
    "param_background_removal = {\n",
    "    'method': 'tophat',\n",
    "    'wnd': 10}\n",
    "```\n",
    "\n",
    "This step attempts to estimate background (everything except the fluorescent signal of in-focus cells) frame by frame and remove it. As with the last step, the first argument to `remove_background` is our video (`varr_mc`), and the second is the `method` to use for background subtraction. There are two methods available: `'uniform'` or `'tophat'`. Both require a single parameter - a window size (`wnd`), which is the third required argument to `remove_background`. The two methods differ in how background is estimated. \n",
    "\n",
    "For `method='tophat'`, a [disk element](http://scikit-image.org/docs/dev/api/skimage.morphology.html#disk) with a radius of `wnd` is created. Then, a [morphological erosion](https://homepages.inf.ed.ac.uk/rbf/HIPR2/erode.htm) using the disk element is applied to each frame, which eats away any bright \"features\" that are smaller than the disk element. Subsequently, a [morphological dilation](https://homepages.inf.ed.ac.uk/rbf/HIPR2/dilate.htm) is applied to the \"eroded\" image, which in theory undoes the erosion except the bright \"features\" that were completely eaten away. The overall effect of this process is to remove any bright feature that is smaller than a disk with radius `wnd`. Thus, when setting `wnd` to the expected size of **largest** cell diamater, this process can give us a good estimation of the background. Pragmatically **10** works well.\n",
    "\n",
    "For `method='uniform'`, a [uniform filter](https://docs.scipy.org/doc/scipy-0.19.0/reference/generated/scipy.ndimage.uniform_filter.html) (basically a two dimensional rolling mean) is applied to each frame. `wnd` controls the window size of the filter, and the result is used as the background. This is only useful if previous steps failed to remove some stable, large scale background, and should be less preferrable than `\"tophat\"` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.output(size=output_size)\n",
    "if interactive:\n",
    "    display(visualize_preprocess(varr_ref.isel(frame=0), remove_background, method=['tophat'], wnd=[10, 15, 20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varr_ref = remove_background(varr_ref, **param_background_removal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the parameters for `save_minian`:\n",
    "\n",
    "```python\n",
    "param_save_minian = {\n",
    "    'dpath': dpath,\n",
    "    'fname': 'minian',\n",
    "    'backend': 'zarr',\n",
    "    'meta_dict': dict(session_id=-1, session=-2, animal=-3),\n",
    "    'overwrite': True}\n",
    "```\n",
    "\n",
    "As was mentioned during the **Setting up** step, the `save_minian` function decides how your data will be saved: `dpath` is the path under which the actual data file will be stored, and `fname` is the file name of the data file. `backend` can be either `'netcdf'` or `'zarr'` -- currently `'netcdf'` is more stable and is the recommended storage option from `xarray`, but it might suffer performance issues when running out-of-core computation. `'zarr'` is designed for parallel and out-of-core computation, and is therefore what is recommended. `meta_dict` is a `dictionary` that is used to construct meta data for the final labeled data structure which can be modified to suit the specific user's data storing structure. `overwrite` is a boolean value (i.e. True/False) controlling whether the data is overwritten when the file already exists. We set it to `True` here so you can easily play with the demo multiple times, but **use extreme caution** with this during actual analysis -- it won't ask again for your confirmation.\n",
    "\n",
    "In particular, here we are saving our minimally-processed video (`varr_ref`) in `DataArray` format. We give it a \"name\" `\"org\"` by calling the [rename](http://xarray.pydata.org/en/stable/generated/xarray.DataArray.rename.html) method on the array, which is `xarray`'s internal naming system that stays with the actual data and will be displayed when you print out the `DataArray`. In practice, give it a name that's human-readable and be sure to not name two pieces of data with the same name (Otherwise an error will occur if you try to combine them in a single dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "varr_ref = varr_ref.chunk(chk)\n",
    "varr_ref = save_minian(varr_ref.rename('org'), **param_save_minian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# motion correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load in from disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load in the data we just saved. We use `'fname'` and `'backend'` from `param_save_minian` since they should be the same and you don't have to specify the same information twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varr_ref = open_minian(dpath,\n",
    "                      fname=param_save_minian['fname'],\n",
    "                      backend=param_save_minian['backend'])['org']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## estimate shifts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the parameters for `estimate shifts`:\n",
    "\n",
    "```python\n",
    "param_estimate_shift = {\n",
    "    'dim': 'frame',\n",
    "    'max_sh': 20}\n",
    "```\n",
    "\n",
    "The idea behind `estimate_shift_fft` is simple: for each frame it calculates a two-dimensional [cross-correlation](https://en.wikipedia.org/wiki/Cross-correlation) between that frame and a template frame using [fft](https://en.wikipedia.org/wiki/Fast_Fourier_transform). The argument `'dim'` specifies along which dimension to run the shift estimation, and should always be set to `'frame'` for this pipeline. To properly calculate the correlation we have to zero-pad the input frame, otherwise our estimation will be biased towards zero shifts. The amount of zero-padding essentially determine the maximum amount of shifts that can be accounted for, and `max_sh` controls this quantity in pixels. The results from `estimate_shift_fft` are saved in a two dimensional `DataArray` called `shifts`, with two labels on the `variable` dimension, representing the shifts along `'height'` and `'width'` directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "shifts = estimate_shifts(varr_ref.sel(subset_mc), **param_estimate_shift)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "shifts = shifts.chunk(dict(frame=chk['frame'])).rename('shifts')\n",
    "shifts = save_minian(shifts, **param_save_minian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualization of shifts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we visualize `shifts` as a fluctuating curve along `frame`s. This is the first time we explicitly use the package [holoviews](http://holoviews.org), which is a really nice package for visualizing data in an interactive manner, and it is highly recommended that you read through the holoviews tutorial to get familiar with its syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts Curve [frame_width=500, tools=['hover'], aspect=2]\n",
    "hv.output(size=output_size)\n",
    "if interactive:\n",
    "    display(hv.NdOverlay(dict(width=hv.Curve(shifts.sel(variable='width')),\n",
    "                              height=hv.Curve(shifts.sel(variable='height')))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply shifts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After determining what each frame's shift from the template is, we use the function `apply_shifts`, which takes as inputs our video (`varr_ref`) and (`shifts`) and returns the movie we want (`Y`). Notably, pixels that are shifted inside the field of view will result in NaN values (`np.nan`) along the edges of our video, and we have to decide what to do with these. The default is to fill them with 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y = apply_shifts(varr_ref, shifts)\n",
    "Y = Y.fillna(0).astype(varr_ref.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively you can leverage the [dropna](http://xarray.pydata.org/en/stable/generated/xarray.DataArray.dropna.html) function to drop them, or [fillna](http://xarray.pydata.org/en/stable/generated/xarray.DataArray.fillna.html) to fill them with a specific value (potentially `varr_mc.min()`)\n",
    "\n",
    "For example, instead of filling the NaN pixels with the nearest available value, you drop these pixels with the following code:\n",
    "```python\n",
    "varr_mc = varr_mc.where(varr_mc.isnull().sum('frame') == 0).dropna('height', how='all').dropna('width', how='all')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualization of motion-correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we visualize the final result of motion correction (`varr_mc`) with `VArrayViewer`. The optional argument `framerate` only controls how the frame slider behaves, not how the data is handled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.output(size=output_size)\n",
    "if interactive:\n",
    "    vaviewer = VArrayViewer(\n",
    "        [varr_ref.rename('before_mc'), Y.rename('after_mc')],\n",
    "        framerate=5,\n",
    "        summary=None,\n",
    "        layout=True)\n",
    "    display(vaviewer.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "Y = Y.chunk(chk)\n",
    "Y = save_minian(Y.rename('Y'), **param_save_minian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate video for motion-correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have some additional visualizations for motion correction. We can generate a video and play it to quickly go through the results. In addition we can look at the max projection before and after motion correction. If there were a lot of translational motion presented in the raw video, we expect the border of cells are much more well-defined, and even some \"different\" cells (due to motion) are \"merged\" together in the max projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "vid_arr = xr.concat([varr_ref, Y], 'width').chunk(dict(height=-1, width=-1))\n",
    "vmax = varr_ref.max().compute().values\n",
    "write_video(vid_arr / vmax * 255, 'minian_mc.mp4', dpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_opts = dict(frame_width=500, aspect=752/480, cmap='Viridis', colorbar=True)\n",
    "(regrid(hv.Image(varr_ref.max('frame').compute(), ['width', 'height'], label='before_mc')).opts(**im_opts)\n",
    " + regrid(hv.Image(Y.max('frame').compute(), ['width', 'height'], label='after_mc')).opts(**im_opts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run CNMF, we first need to generate an initial estimate of where our cells are likely to be and what their temporal activity is likely to look like. The whole initialization section is adapted from the [MIN1PIPE](https://github.com/JinghaoLu/MIN1PIPE) package. See their [paper](https://www.cell.com/cell-reports/fulltext/S2211-1247(18)30826-X) for full details about the theory. Here we only give enough information so that we can select parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load in from disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we want to do is open up the dataset we just saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "minian = open_minian(dpath,\n",
    "                     fname=param_save_minian['fname'],\n",
    "                     backend=param_save_minian['backend'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we get the movie (`Y`) from the dataset, calculate a max projection that will be used later, and generate a flattened version of our video (`Y_flt`), where the original dimemsions `'height'` and `'width'` are flattened as one dimension `spatial`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = minian['Y'].astype(np.float)\n",
    "max_proj = Y.max('frame').compute()\n",
    "Y_flt = Y.stack(spatial=['height', 'width'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generating over-complete set of seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to initialize the **seeds**. Recall the parameters:\n",
    "\n",
    "```python\n",
    "param_seeds_init = {\n",
    "    'wnd_size': 2000,\n",
    "    'method': 'rolling',\n",
    "    'stp_size': 1000,\n",
    "    'nchunk': 100,\n",
    "    'max_wnd': 15,\n",
    "    'diff_thres': 3}\n",
    "```\n",
    "\n",
    "The idea is that we select some subset of frames, compute a max projection of those frames, and find the local maxima of that max projection. We keep repeating this process and putting together all the local maxima we get along the way until we get an overly-complete set of local maxima/bright-spots, which are the potential locations of cells. We call them **seeds**. The assumption here is that the center of cells are brighter than their surroundings on some, but not necessarily all, frames. The first and only required argument `seeds_init` takes is the video array we want to process (here, `Y`). There are four additional arguments controlling how we subset the frames: `wnd_size` controls the window size of each chunk (*i.e* the number of frames in each chunk); `method` can be either `'rolling'` or `'random'`. For `method='rolling'`, the moving window will roll along `frame`, whereas for `method='random'`, chunks with `wnd_size` number of frames will be randomly selected; `stp_size` is only used if `method='rolling'`, and is the step-size of the rolling window, or in other words, the distance between the **center** of each rolling window. For example, if `wnd_size=100` and `stp_size=200`, the windows will be as follows: **(0, 100)**, **(200, 300)**, **(400, 500)** *etc.* Obviously that was a **bad** choice since you probably want the windows to overlap or you will miss cells. `nchunk` is only used if `method='random'`, and is the number of random chunks we will draw. Additionally we have two parameters controlling how the local maxima are found. `'max_wnd'` controls the window size within which a single pixel will be choosen as local maxima. In order to capture cells with all sizes, we actually find local maximas with different window size and merge all of them, starting from **2** all the way up to `'max_wnd'`. Hence `'max_wnd'` should be the radius of the **largest** cell you want to detect. Finally in order to get rid of local maxima with very little fluctuation, we set a `'diff_thres'` which is the minimal fluorescent diffrence of a seed across `frame`s. Since the linear scale of the raw data is preserved, we can set this threshold emprically.\n",
    "\n",
    "<div class=\"alert alert-info\">  \n",
    "The default values of `seeds_init` usually work fairly well for a dense region like CA1. If you are working with deep brain region with sparse cells, try to increase wnd_size and stp_size to make the following <strong>seeds</strong> cleaning steps faster and cleaner.\n",
    "</div> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "seeds = seeds_init(Y, **param_seeds_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the seeds as points overlaid on top of the `max_proj` image. Each white dot is a seed and could potentially be the location of a cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.output(size=output_size)\n",
    "visualize_seeds(max_proj, seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## peak-noise-ratio refine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We further refine seeds based upon their temporal activity.  This requires that we separate our signal based upon frequency, and this also brings us to the most powerful and important aspect of this pipeline -- parameter exploring.  We are going to take a few example seeds and separate their activity based upon a few frequencies, and we will then view the results and select a frequency which we beleive best separates signal from noise. \n",
    "\n",
    "This will seem to be the most complicated chunk of code so far, but it is important to read through, since we will see similar things later and it is a very powerful piece of code that can help you visualize a lot. The basic idea is we run some function on a small subset of data using different parameters within a `for` loop, and after that visualize the results using `holoviews`. Note that interactive mode needs to be set as `True` for parameter exploring steps like this to work.\n",
    "\n",
    "The goal of this specific piece of code is to determine the \"frequency\" at which we can best seperate our signal from noise, which is an important parameter used at various places below. We will go line by line: First we create a `list` of frequencies we want to try out -- `noise_freq_list`. The \"frequency\" values here are a proportion of your **sampling rate**. Note that if you have temporally downsampled, the proportion here is relative to the downsampled rate. Then we randomly select 6 seeds from `seeds_gmm` and call them `example_seeds`, which in turn help us pull out the temporal traces from the movie `Y_flt`.  The traces of the `example_seeds` are assigned to `example_trace`. We then create an empty dictionary `smooth_dict` to store the resulting visualizations. After initializing these variables, we use a `for` loop to iterate through `noise_freq_list`, with one of the values from the list as `freq` during each iteration. Within the loop, we run `smooth_sig` twice on `example_trace` with the current `freq` we are testing out. The low-passed result is assigned to `trace_smth_low`, while the high-pass result is assigned to `trace_smth_high`. Then we make sure to actually carry-out the computation by calling the `compute` method on the resulting `DataArray`s. Finally, we turn the two traces into visualizations: we construct interactive line plots ([hv.Curve](http://holoviews.org/reference/elements/bokeh/Curve.html)s) from them and put them in a container called a [hv.HoloMap](http://holoviews.org/reference/containers/bokeh/HoloMap.html). Again if you are confused about how the visualization works, you can check out [the tutorial](http://holoviews.org/getting_started/Introduction.html). After that we store the whole visualization in `smooth_dict`, with the keys being the `freq` and values corresponding to the result of this iteration.\n",
    "\n",
    "<div class=\"alert alert-info\"> \n",
    "Here you can edit the values that you want to test in the noise_freq_list.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if interactive:\n",
    "    noise_freq_list = [0.005, 0.01, 0.02, 0.06, 0.1, 0.2, 0.3, 0.45]\n",
    "    example_seeds = seeds.sample(6, axis='rows')\n",
    "    example_trace = (Y_flt\n",
    "                     .sel(spatial=[tuple(hw) for hw in example_seeds[['height', 'width']].values])\n",
    "                     .assign_coords(spatial=np.arange(6))\n",
    "                     .rename(dict(spatial='seed')))\n",
    "    smooth_dict = dict()\n",
    "    for freq in noise_freq_list:\n",
    "        trace_smth_low = smooth_sig(example_trace, freq)\n",
    "        trace_smth_high = smooth_sig(example_trace, freq, btype='high')\n",
    "        trace_smth_low = trace_smth_low.compute()\n",
    "        trace_smth_high = trace_smth_high.compute()\n",
    "        hv_trace = hv.HoloMap({\n",
    "            'signal': (hv.Dataset(trace_smth_low)\n",
    "                       .to(hv.Curve, kdims=['frame'])\n",
    "                       .opts(frame_width=300, aspect=2, ylabel='Signal (A.U.)')),\n",
    "            'noise': (hv.Dataset(trace_smth_high)\n",
    "                      .to(hv.Curve, kdims=['frame'])\n",
    "                      .opts(frame_width=300, aspect=2, ylabel='Signal (A.U.)'))\n",
    "        }, kdims='trace').collate()\n",
    "        smooth_dict[freq] = hv_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all the loops are done, we put together a holoviews plot (`hv.HoloMap`) from `smooth_dict`, and we specify that we want our traces to `overlay` each other along the `'trace'` dimension while being laid out along the `'spatial'` dimension. The result turns into a nicely animated interactive plot, from which we can determine the frequency that best separates noise and signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.output(size=output_size)\n",
    "if interactive:\n",
    "    hv_res = (hv.HoloMap(smooth_dict, kdims=['noise_freq']).collate().opts(aspect=2)\n",
    "              .overlay('trace').layout('seed').cols(3))\n",
    "    display(hv_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having determined the frequency that best separates signal from noise, we move on the next step of seeds refining. Recall the parameters:\n",
    "\n",
    "```python\n",
    "param_pnr_refine = {\n",
    "    'noise_freq': 0.06,\n",
    "    'thres': 1,\n",
    "    'med_wnd': None}\n",
    "```\n",
    "\n",
    "`pnr_refine` stands for \"peak-to-noise ratio\" refine. The \"peak\" and \"noise\" here are defined differently from before. First we seperate/filter the temporal signal for each seed based on frequency -- the signals composed of the lower half of the frequency are regarded as **real** signals, while the higher half of the frequencies is presumably **noise** (\"half\" being relative to [Nyquist frequency](https://en.wikipedia.org/wiki/Nyquist_frequency)). Then we take the peak-to-valley value (really just **max** minus **min**, or, [np.ptp](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.ptp.html)) for both the **real** signal and **noise** signal. Then, \"peak-to-noise ratio\" is the ratio between the `np.ptp` values of **real** and **noise** signals. So, the critical assumption here is that real cell activity is of lower frequency while noise is of a higher frequency, and they seperate at approximately half the Nyquist frequency, or, one-fourth of the sampling frequency of the video.  Moreover, we don't want those \"seeds\" whose **real** signals are buried in **noise**. If these assumptions does not suit your recordings - for example, if you have a really low sampling rate, or if your video are unavoidably noisy - consider skipping this step. The function `pnr_refine` takes in `varr` and `seeds` as its first two arguments; the `noise_freq` that best separates signal and noise, which hopefully has been determined from the previous cell; and `thres`, a threshold for \"peak-to-noise ratios\" below which seeds will be discarded. Pragmatically `thres=1` works fine and makes sense. You can also use `thres='auto'`, where a gaussian mixture model with 2 components will be run on the peak-to-noise ratios and seeds will be selected if they belong to the \"higher\" gaussian. `med_wnd` is the window size of the median filter that gets passed in as `size` in [`scipy.ndimage.filters.median_filter`](https://docs.scipy.org/doc/scipy-0.16.1/reference/generated/scipy.ndimage.filters.median_filter.html). This is only useful in rare cases where the signal of some seeds assume a huge change in baseline fluorescence and it is not desirable to keep such seeds. In this case the median-filtered signal is subtracted from the original signal to get rid of the artifact. In other cases `'med_wnd'` should be left to `None`.\n",
    "\n",
    "Now we can use the previous visualization result to pick the best frequency!\n",
    "\n",
    "![pnr_param](img/pnr_param_v2.png)\n",
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "What we are looking for here is the frequency that can seperate <strong>real</strong> signal and <strong>noise</strong> the best, which means the left panel in the example trace, with the `noise_freq` = 0.005, is not ideal. In the mean time, we also don't want the signal bands to be overly thick which is showing in the right panel with the `noise_freq` = 0.45. Thus, the middle trace with `noise_freq` = 0.05 best suits the needs! \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-info\">  \n",
    "Now, say you already found your parameters, it's time now to pass them in! Either go back to initial parameters setting step and modify them there, or call the parameter here and change its value/s accordingly. \n",
    "</div>\n",
    "\n",
    "For example, if you want to change `noise_freq` to 0.05, and start using median filter equal to 501 here:\n",
    "```python\n",
    "param_pnr_refine['noise_freq'] = 0.05\n",
    "param_pnr_refine['med_wnd'] = 501\n",
    "```\n",
    "Finally, run the following code cell to further clean the seeds:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds, pnr, gmm = pnr_refine(Y_flt, seeds.copy(), **param_pnr_refine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here in the belowing code cell we will visualize the gmm fit, but **only** when you chose `thres='auto'` before. The x axis here is pnr ratio value, and the x value of the intersection of blue and red curve is the auto chose threshold, everything below this threshold will be seen as noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gmm:\n",
    "    display(visualize_gmm_fit(pnr, gmm, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again we can visualize seeds that's taken out during this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.output(size=output_size)\n",
    "visualize_seeds(max_proj, seeds, 'mask_pnr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still, white dots are accepted seeds and red dots are taken out. \n",
    "<div class=\"alert alert-info\"> \n",
    "if you see seeds that you believe should be cells have been taken out here, either skip this step or try lower the threshold a bit. You can also use the individual trace ploting method we discussed at the end of gmm_refine part to look into specific seed. \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ks refine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ks_refine` refines the seeds using [Kolmogorov-Smirnov test](https://en.wikipedia.org/wiki/Kolmogorov–Smirnov_test). Recall the parameters:\n",
    "\n",
    "```python\n",
    "param_ks_refine = {\n",
    "    'sig': 0.05}\n",
    "```\n",
    "\n",
    "The idea is simple: if a seed corresponds to a cell, its fluorescence intensity across frames should be somewhat [bimodal](https://en.wikipedia.org/wiki/Multimodal_distribution), with a large normal distribution representing silence/little activity, and another peak representing when the seed/cell is active. Thus, we can carry out KS test on the intensity distribution of each seed, and keep only the seeds where the null hypothesis (that the fluoresence is simply a normal distribution) is rejected. `ks_refine` takes in `varr` and `seeds` as its first two arguments, then a `sig` which is the significance level at which the null hypothesis is rejected (defaulted to **0.05**). \n",
    "<div class=\"alert alert-info\"> \n",
    "In practice, we have found this step tends to take away real cells when video are very short (for example, the one that comes with this package under \"./demo_movies\"). This is likely because the number of \"active\" frames is too small. Feel free to skip this step if you encounter the same situation.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "seeds = ks_refine(Y_flt, seeds[seeds['mask_pnr']], **param_ks_refine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.output(size=output_size)\n",
    "visualize_seeds(max_proj, seeds, 'mask_ks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, much of our refined seeds likely reflect the position of an actual cell.  However, we are likely to still have multiple seeds per cell, which we want to avoid.  Here we discard redudant seeds through a process of merging.\n",
    "\n",
    "Recall the parameters:\n",
    "\n",
    "```python\n",
    "param_seeds_merge = {\n",
    "    'thres_dist': 5,\n",
    "    'thres_corr': 0.7,\n",
    "    'noise_freq': .06'}\n",
    "```\n",
    "\n",
    "The function `seeds_merge` attempts to merge seeds together which potentially come from the same cell, based upon their spatial distance and temporal correlation. Specifically, `thres_dist` is the threshold for euclidean distance between pairs of seeds, in pixels, and `thres_corr` is the threshold for pearson correlation between pairs of seeds. In addition, it's very beneficial to smooth the signals before running the correlation, and again `noise_freq` determines how smoothing should be done. In addition to feeding in a number, such as the noise frequency you defined earlier during `seeds_refine_pnr`, you can also use `noise_freq='envelope'`.  When `noise_freq='envelope'`, a hilbert transform will be run on the temporal traces of each seed and the correlation will be calculated on the envelope signal. Any pair of seeds that are within `thres_dist` **and** has a correlation higher than `thres_corr` will be merged together, such that only the seed with maximum intensity in the max projection of the video will be kept. Thus `thres_dist` should be the expected size of cells and `thres_corr` should be relatively high to avoid over-merging.\n",
    "\n",
    "<div class=\"alert alert-info\"> \n",
    "Potentially we could pick out multiple seeds that are actually within one cell, but we want to avoid that as much as possible to have a clean start for CNMF later, you can try lower the thres_corr or raise up the thres_dist to merge more cells. Ideally, you want to see only one accepted seed (white dot) within each cell. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "seeds_final = seeds[seeds['mask_ks']].reset_index(drop=True)\n",
    "seeds_mrg = seeds_merge(Y_flt, seeds_final, **param_seeds_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.output(size=output_size)\n",
    "visualize_seeds(max_proj, seeds_mrg, 'mask_mrg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize spatial and temporal matrices from seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up till now, the seeds we have are only one-pixel dots. In order to kick start CNMF we need something more like the spatial footprint (`A`) and temporal activities (`C`) of real cells. Thus we need to `initilalize` `A` and `C` from the seeds we have (`seeds_mrg`). Recall the parameters:\n",
    "\n",
    "```python\n",
    "param_initialize = {\n",
    "    'thres_corr': 0.8,\n",
    "    'wnd': 10,\n",
    "    'noise_freq': .06\n",
    "}\n",
    "```\n",
    "\n",
    "To obtain the initial spatial matrix `A`, for each seed, we simply use a Pearson correlation between the seed and surrounding pixels. Apparantly cacluating correlation with all other pixels for every seed is time-consuming and unnecessary. `'wnd'` controls the window size for calculating the correlation, and thus is the maximum possible size of any spatial footprint in the initial spatial matrix. At the same time we do not want pixels with low correlation value to influence our estimation of temporal signals, thus a `'thres_corr'` is also implemented where only pixels with correlation above this threshold are kept. After generating `A`, for each seed, we calculate a weighted average of pixels around the seed, where the weight are the initial spatial footprints in `A` we just generated. We use this weighted average as the initial estimation of temporal activities for each units in `C`. Finally, we need two more terms: `b` and `f`, representing the spatial footprint and temporal dynamics of the **background**, respectively. Since usually the backgrounds are already removed at this stage, we provide a very simple estimation of remaining background -- we simply mask `Y` with the spatial footprints of units in `A`, that is, we only keep pixels that does not appear in the spatial foorprints of any units. We calculate a mean projection across `frame`s and use as `b`, and we calculate mean fluorescence along `frame`s and use as `f`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "A, C, b, f = initialize(Y, seeds_mrg[seeds_mrg['mask_mrg']], **param_initialize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we visualize the result of our initialization by plotting a projection of the spatial matrix `A`, a raster of the temporal matrix `C`, as well as background terms `b` and `f`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_opts = dict(frame_width=500, aspect=A.sizes['width']/A.sizes['height'], cmap='Viridis', colorbar=True)\n",
    "cr_opts = dict(frame_width=750, aspect=1.5*A.sizes['width']/A.sizes['height'])\n",
    "(regrid(hv.Image(A.sum('unit_id').rename('A').compute(), kdims=['width', 'height'])).opts(**im_opts)\n",
    " + regrid(hv.Image(C.rename('C').compute(), kdims=['frame', 'unit_id'])).opts(cmap='viridis', colorbar=True, **cr_opts)\n",
    "  + regrid(hv.Image(b.rename('b').compute(), kdims=['width', 'height'])).opts(**im_opts)\n",
    " + datashade(hv.Curve(f.rename('f').compute(), kdims=['frame']), min_alpha=200).opts(**cr_opts)\n",
    ").cols(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we save the results in the dataset. Note here that we change the name of a dimension by writing `rename(unit_id='unit_id_init')`. The name of this dimension is changed as a precaution, since the size of the dimension `unit_id` will likely change in the next section **CNMF**. During CNMF, most likely units will be merged, and there will be conflicts if we save other variables with dimension `unit_id` that have different coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "A = save_minian(A.rename('A_init').rename(unit_id='unit_id_init'), **param_save_minian)\n",
    "C = save_minian(C.rename('C_init').rename(unit_id='unit_id_init'), **param_save_minian)\n",
    "b = save_minian(b.rename('b_init'), **param_save_minian)\n",
    "f = save_minian(f.rename('f_init'), **param_save_minian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section assume you already have some knowledge about using CNMF as a method of extracting neural activities from video. If not, it is recommended that you read [the paper](https://www.sciencedirect.com/science/article/pii/S0896627315010843), to get a broad understanding of the problem and proposed solution.\n",
    "\n",
    "As a quick reminder, here is the essential idea of CNMF: We believe our movie, `Y`, with dimensions `height`, `width` and `frame`, can be written in (and thus broken down as) the following equation: $$\\mathbf{Y} = \\mathbf{A} \\cdot \\mathbf{C} + \\mathbf{b} \\cdot \\mathbf{f} + \\epsilon$$ where `A` is the spatial footprint of each unit, with dimension `height`, `width` and `unit_id`; `C` is the temporal activities of each unit, with dimension `unit_id` and `frame`; `b` and `f` are the spatial footprint and temporal activities of some background, respectively; and $\\epsilon$ is the noise. Note that strictly speaking, matrix multiplication is usually only defined for two dimensional matrices, but our `A` here has three dimensions, so in fact we are taking the [tensor product](https://en.wikipedia.org/wiki/Tensor_product) of `A` and `C`, reducing the dimension `unit_id`. This might seem to complicate things (compared to just treating `height` and `width` as one flattened `spatial` dimension), but it ends up making some sense. When you take a dot product of any two \"matrices\" on a certain **dimension**, all that is happening is a **product** followed by a **sum** -- you take the product for all pairs of matching numbers coming from the two \"matrices\", where \"match\" is defined by their index along said dimension, and then you take the sum of all those products along the dimension. Thus when we take the tensor product of `A` and `C`, we are actually multiplying all those numbers in dimension `height`, `width` and `frame`, matched by `unit_id`, and then take the sum. Conceptually, for each unit, we are weighting the spatial footprint (`height` and `width`) by the fluorecense of that unit on given `frame`, which is the **product**, and then we are overlaying all units together, which is the **sum**. With that, the equation above is trying to say that our movie is made up of a weighted sum of the spatial footprint and temporal activities of all units, plus some background and noise.\n",
    "\n",
    "Now, there is another rule about `C` that separates it from background and noise, and saves it from being just some random matrix that happens to fit well with the data (`Y`) without having any biological meaning. This rule is the second essential idea of CNMF: each \"row\" of `C`, which is the temporal trace for each unit, should be described as an [autoregressive process](https://en.wikipedia.org/wiki/Autoregressive_model) (AR process), with a parameter `p` defining the **order** of the AR process: $$ c(t) = \\sum_{i=0}^{p}\\gamma_i c(t-i) + s(t) + \\epsilon$$ where $c(t)$ is the calcium concentration at time (`frame`) $t$, $s(t)$ is spike/firing rate at time $t$ (what we actually care about), and $\\epsilon$ is noise. Basically, this equation is trying to say that at any given time $t$, the calcium concentration at that moment $c(t)$ depends on the spike at that moment $s(t)$, as well as its own history up to `p` time-steps back $c(t-i)$, scaled by some parameters $\\gamma_i$s, plus some noise $\\epsilon$. Another intuition of this equation comes from looking at different `p`s: when `p=0`, the calcium concentration is an exact copy of the spiking activities, which is probably not true; when `p=1`, the calcium concentration has an instant rise in response to a spike followed by an exponential decay; when `p=2`, calcium concentration has some rise time following a spike and an exponential decay; when `p>2`, more convoluted waveforms start to emerge.\n",
    "\n",
    "With all this in mind, CNMF tries to find the spatial matrix (`A`) and temporal activity (`C`) (along with `b` and `f`) that best describe `Y`. There are a few more important practical concerns: Firstly we cannot solve this problem in one shot -- we need to iteratively and separately update `A` and `C` to approach the true solution -- and  we need something to start with (that is what **initilization** section is about). Surprisingly often times 2 iterative steps after our initialization seem to give good enough results, but you can always add more iterations (and you should be able to easily do that after reading the comments). Secondly, by intuition you may define \"best describe `Y`\" as the results that minimize the noise $\\epsilon$ (or residuals, if you will). However we have to control for the [sparsity](https://en.wikipedia.org/wiki/Sparse_matrix) of our model as well, since we do not want every little random pixel that happens to correlate with a cell to be counted as part of the spatial footprint of the cell (non-sparse `A`), nor do we want a tiny spike at every frame trying to explain every noisy peak we observe (non-sparse `C`). Thus, the balance between fidelity (minimizing error) and sparsity (minimizing non-zero entries) is an important idea for both the spatial and temporal update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load in our data from previous steps. `'unit_id'` is renamed as a precaution, mentioned at the end of the **initialization** section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "minian = open_minian(dpath,\n",
    "                     fname=param_save_minian['fname'],\n",
    "                     backend=param_save_minian['backend'])\n",
    "Y = minian['Y'].astype(np.float)\n",
    "A_init = minian['A_init'].rename(unit_id_init='unit_id')\n",
    "C_init = minian['C_init'].rename(unit_id_init='unit_id')\n",
    "b_init = minian['b_init']\n",
    "f_init = minian['f_init']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## estimate spatial noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to performing CNMF's first spatial update, we need to get a sense of how much noise is expected, which we will then feed into CNMF. To do so, we compute an fft-transform for every pixel independently, and estimate noise from its [power spectral density](https://en.wikipedia.org/wiki/Spectral_density). Recall the parameters:\n",
    "\n",
    "```python\n",
    "param_get_noise = {\n",
    "    'noise_range': (0.06, 0.5),\n",
    "    'noise_method': 'logmexp'}\n",
    "```\n",
    "\n",
    "Note that the number in `noise_range` is relative to the sampling frequency, so **0.5** actually represents the Nyquist frequency and is the highest you can go as far as fft is concerned. Thus **(0.25, 0.5)** is the higher frequency half of the signal. After choosing `noise_range`, we have to decide how to collapse across different frequencies to get a single number of noise power for each pixel. Three `noise_method`s are availabe: `noise_method='mean'` and `noise_method='median'` will use the mean and median across all `freq` as the estimation of noise for each pixel. `noise_method='logmexp'`is a bit more complicated -- the equation is as follows: $sn = \\exp( \\operatorname{\\mathbb{E}}[\\log psd] )$ where $\\exp$ is the [exponential function](Exponential_function), $\\operatorname{\\mathbb{E}}$ is the [expectation operator](https://en.wikipedia.org/wiki/Expected_value) (mean), $\\log$ is [natural logarithm](https://en.wikipedia.org/wiki/Natural_logarithm), $psd$ is the spectral density of noise for any pixel, and $sn$ is the resulting estimation of noise power. It is recommended to keep `noise_method='logmexp'` since this is the default behavior of the [CaImAn](https://github.com/flatironinstitute/CaImAn) package.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\">  \n",
    "In order to define the lower bound of <strong>noise_range</strong> (the upper bound can be left equal to 0.5), examine the PSD plot and define the frequency value (again, this is actually a proportion of your sampling rate), where power has dropped off across all pixels (i.e., <strong>spatial</strong>).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sn_spatial = get_noise_fft(Y, **param_get_noise).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test parameters for spatial update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now do some parameter exploring before actually performing the first spatial update.  We do this because we do not want to do a 10-minute spatial update only to find the selected parameters do not produce nice results. For parameter exploration, we will analyze a very small subset of data so that we can quickly examine the influence of various paramater values. Here, we randomly select 10 units from `A_init.coords['unit_id']` with the help of [`np.random.choice`](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.choice.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interactive:\n",
    "    units = np.random.choice(A_init.coords['unit_id'], 10, replace=False)\n",
    "    units.sort()\n",
    "    A_sub = A_init.sel(unit_id=units).persist()\n",
    "    C_sub = C_init.sel(unit_id=units).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we again perform parameter exploration using a `for` loop and visualization with help of `dict` and `holoviews`, only this time we use a convenient function, `visualize_spatial_update` from `minian`, to handle all the visualization details. For now, the sparseness penalty (`sparse_penal`) is only one parameter in `update_spatial` that we are interested in playing with, but there is nothing stopping you from adding more. Discussion of all the parameters for `update_spatial` will follow soon.\n",
    "\n",
    "<div class=\"alert alert-info\">  \n",
    "Here, you can simply <strong>add</strong> the values that you want to test or <strong>delete</strong> the values you are not interested in from <strong>spar_ls</strong>. Pragmatically, the range of 0.05 to 1  is reasonable.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if interactive:\n",
    "    sprs_ls = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    A_dict = dict()\n",
    "    C_dict = dict()\n",
    "    for cur_sprs in sprs_ls:\n",
    "        cur_A, cur_b, cur_C, cur_f = update_spatial(\n",
    "            Y, A_sub, b_init, C_sub, f_init,\n",
    "            sn_spatial, dl_wnd=param_first_spatial['dl_wnd'], sparse_penal=cur_sprs)\n",
    "        if cur_A.sizes['unit_id']:\n",
    "            A_dict[cur_sprs] = cur_A.compute()\n",
    "            C_dict[cur_sprs] = cur_C.compute()\n",
    "    hv_res = visualize_spatial_update(A_dict, C_dict, kdims=['sparse penalty'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we actually plot the visualization `hv_res`. What you should expect here will be explained later along with what `sparse_penal` actually does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.output(size=output_size)\n",
    "if interactive:\n",
    "    display(hv_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## first spatial update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the idea behind `update_spatial`. Recall the parameters:\n",
    "\n",
    "```python\n",
    "param_first_spatial = {\n",
    "    'dl_wnd': 10,\n",
    "    'sparse_penal': 0.01,\n",
    "    'update_background': True,\n",
    "    'normalize': True,\n",
    "    'zero_thres': 'eps'}\n",
    "```\n",
    "\n",
    "To reiterate, the big picture is that given the data (`Y`) and our units' activity (`C`) from previous the update (which is `C_init`), we want to find the spatial footprints (`A`) such that 1. the **error** `Y - A.dot(C, 'unit_id')` is as small as possible, and 2. the [**l1-norm**](http://mathworld.wolfram.com/L1-Norm.html) of `A` is as small as possible. Here the **l1-norm** is a proxy to control for the sparsity of `A`. Ideally to promote sparsity we want to control for the number of non-zero entries in `A`, which is the [l0-norm](https://en.wikipedia.org/wiki/Lp_space#When_p_=_0). However optimizing for the l0-norm is typically [computationally hard to do](https://stats.stackexchange.com/questions/269298/why-do-we-only-see-l-1-and-l-2-regularization-but-not-other-norms), and it is usually good enough to use **l1-norm** instead as a proxy.\n",
    "\n",
    "Now, in theory we want to update every entry in  `A` iteratively with the above two goals in mind. However, updating that amount of numbers in `A` is still computationally very demanding, and it is much better if we can breakdown our big problem into smaller chunks that can be parallelized (making things much faster). **CNMF** is all about solving the issues caused by overlapping neurons, so it is best to keep the dependency across units (along dimension `unit_id`) and update these entries together. However, it should be fine to treat each pixel as independent and update different pixels separately (in parallel). Thus, our new, \"smaller\" problem is: for each pixel, find the corresponding pixel in `A`, across all `unit_id`, that give us smallest **l1-norm** as well as smallest **error** when multiplied by `C`. In equation form, this is:\n",
    "\n",
    "$$\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "& \\underset{A_{ij}}{\\text{minimize}}\n",
    "& & \\left \\lVert Y_{ij} - A_{ij} \\cdot C \\right \\rVert ^2 + \\alpha \\left \\lvert A_{ij} \\right \\rvert \\\\\n",
    "& \\text{subject to}\n",
    "& & A_{ij} \\geq 0 \n",
    "\\end{aligned}\n",
    "\\end{equation*}$$\n",
    "\n",
    "where we use $A_{ij}$ to represent one pixel in `A`, like `A.sel(height=i, width=j)`, which will only have one dimension left: `unit_id`. Similarly $Y_{ij}$ is the corresponding pixel in `Y` which will only have the dimension `frame` left. Thus, $\\left \\lVert Y_{ij} - A_{ij} \\cdot C \\right \\rVert ^2$ is our **error** term and $\\left \\lvert A_{ij} \\right \\rvert$ is our **l1-norm**. Moreover, we put these two terms together as a unitary target function/common goal to be minimized, with $\\alpha$ controlling the balance between them. This balance can be seen by considering the impact of $\\alpha$: the higher the value of $\\alpha$, the greater the contribution the **l1-norm** term makes to the common goal (target function), the more penalty/emphasis you place on sparsity, and as a result, the more sparse `A` will be. The determination of the exact value of $\\alpha$ is rather complicated, but the parameter we have for `update_spatial` is relative, where `alpha=1` corresponds to the default behavior of **CaImAn** package, and is usually a good place to start testing. \n",
    "\n",
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "Here is a good place to bring back the parameter exploring visualization results from the previous step and make sense of them! Pragmatically, relatively small values of <strong>sparse_penal</strong> have very little impact on the resulting <strong>A</strong>, but once you hit a large enough value, you will start to see units getting dimmer, sometimes completely disappearing. You might think this is the sparsity penalty in action, but from experience this is usually a case you want to <strong>avoid</strong>. After all, <strong>update_spatial</strong> has no way to differentiate noise from cells other than their corresponding temporal trace. Thus, you do not want <strong>update_spatial</strong> to take out cells for you unless you strongly trust the temporal traces (which you shouldn't for now since it's the first update and the temporal traces we have are merely weighted means of the original movie). If you are still puzzled about how to pick the right <strong>sparse_panel</strong> from the previous parameter exploring step, below we provide an illustrative example.\n",
    "</div>    \n",
    "\n",
    "![1st spatial update param exploring](img/sparse_panel_spatial_update.PNG)\n",
    "\n",
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "What you are seeing here is parameter testing of the first spatial update. The left panel is the result with <strong>sparse_penal = 0.01</strong>, the middle panel the results with <strong>sparse_penal = 0.3</strong>, and the right the results with <strong>sparse_penal = 1</strong>. Ideally, we want the <strong>Binary Spatial Matrix</strong> to best mimic the real spatial footprint, which also means, they should be shaped like a cell. Thus, in this specific example, <strong>sparse_panel = 0.01</strong> (left penal) is not a good choice. Secondly, we also don't want to actually get rid of cells by using a high sparse panelty value at this step, which means <strong>sparse_panel = 1</strong> (right penal) is not good as well. Thus, <strong>sparse_panel = 0.3</strong> (middle panel) is a fairly good parameter to choose here.\n",
    "</div>\n",
    "\n",
    "There is yet another parameter, `dl_wnd`, that is relevant to practical consideration. Recall that we are updating $A_{ij}$ for our \"small\" problem, which has the dimension `unit_id` and has `A.sizes['unit_id]` number of entries (that is, the number of units). This is computationally feasible, but still a lot, especially when you do this for all pixels. One way to reduce computational demand is to leave out certain units when updating certain pixels -- in particular, it does not make sense to consider a unit that is supposed to be at the top left corner of the field of view when we update a pixel in the bottom right corner. In other words, for each pixel, we solve the \"small\" problem with only a subset of all potential units, thus hugely increasing the speed of `update_spatial`. This is where `A_init` comes into play (actually the only place it is used -- we do not need `A` at all for the update itself). We compute a morphological dilation, like that used during [background removal](#background-removal), on `A_init`, unit by unit, with window size `dl_wnd`, and we use the result as a **masking matrix**. Then, during the actual update of any given pixel, only units that have a non-zero value at the corresponding pixel in the **masking matrix** will be considered for update. In other words, we are allowing each unit to expand from `A_init` up to a distance of `dl_wnd`, and killing off any possibility beyond that range. The rationale of using `dl_wnd` here is that even if for some reason we have only one non-zero pixel representing the center of a certain unit in `A_init`, that unit can potentially expand to a full size cell, but anything beyond that would probably be either part of other cells or random noise. Thus, we want to set `dl_wnd`to approximately the radius of the largest cell to help ensure we get a clean footprint for all cells.\n",
    "\n",
    "Then we have a boolean parameter, `update_background`, controlling whether we want to update the background in this step. This is the only place in the pipeline that the background will be updated, and the way it is updated is by essentially treating `b` as another `unit` and updating it according to the temporal activity `f`. Pragmatically since the morphology-based [background removal](#backgroun-removal) works so well at cleaning the backgrounds, this updating has little impact on the result.\n",
    "\n",
    "Due to the actual implementation of the optimization method, it is hard for the computer to set some variables to absolutely zero. Instead, we usually have a very small float numbers in place of zeros. `zero_thres` solves this by thresholding all the values and setting anything below `zero_thres` to zero. You want to use a very small number for `zero_thres`. Setting `zero_thres='eps'` will use the [machine epsilon](https://en.wikipedia.org/wiki/Machine_epsilon)(the smallest non-negative number a machine can represent) of current datatype.\n",
    "\n",
    "Finally, we have an additional step after everything: normalization so that the spatial footprint of each unit has unit-norm. In practice we found that normalizing the result helps promoting the numerical stability of the algorithm, and enable us to interpret the spatial footprints as \"weights\" on each pixel so that the temporal activities are in the same scale space across units and can be compared. However normlizing spatial footprint for each unit does not preserve the relationship between overlapping cells in terms of their relative contribution to the activities of shared pixels. If such interpretation is critical for your downstream analysis, consider turning this off.\n",
    "\n",
    "`update_spatial` takes in the original data (`Y`), the initial spatial footprint for units and background (`A` and `b`, respectively), the initial temporal trace for units and background (`C` and `f`, respectively), and the estimated noise on each pixel (`sn`), in that order. Optional arguments are `sparse_penal`, `dl_wnd`, `update_background`, `post_scal` and `zero_thres`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "A_spatial, b_spatial, C_spatial, f_spatial = update_spatial(\n",
    "    Y, A_init, b_init, C_init, f_init, sn_spatial, **param_first_spatial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.output(size=output_size)\n",
    "opts = dict(plot=dict(height=A_init.sizes['height'], width=A_init.sizes['width'], colorbar=True), style=dict(cmap='Viridis'))\n",
    "(regrid(hv.Image(A_init.sum('unit_id').compute().rename('A'), kdims=['width', 'height'])).opts(**opts).relabel(\"Spatial Footprints Initial\")\n",
    "+ regrid(hv.Image((A_init.fillna(0) > 0).sum('unit_id').compute().rename('A'), kdims=['width', 'height']), aggregator='max').opts(**opts).relabel(\"Binary Spatial Footprints Initial\")\n",
    "+ regrid(hv.Image(A_spatial.sum('unit_id').compute().rename('A'), kdims=['width', 'height'])).opts(**opts).relabel(\"Spatial Footprints First Update\")\n",
    "+ regrid(hv.Image((A_spatial > 0).sum('unit_id').compute().rename('A'), kdims=['width', 'height']), aggregator='max').opts(**opts).relabel(\"Binary Spatial Footprints First Update\")).cols(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOT THIS ONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.output(size=output_size)\n",
    "opts_im = dict(plot=dict(height=b_init.sizes['height'], width=b_init.sizes['width'], colorbar=True), style=dict(cmap='Viridis'))\n",
    "opts_cr = dict(plot=dict(height=b_init.sizes['height'], width=b_init.sizes['height'] * 2))\n",
    "(regrid(hv.Image(b_init, kdims=['width', 'height'])).opts(**opts_im).relabel('Background Spatial Initial')\n",
    " + datashade(hv.Curve(f_init, kdims=['frame'])).opts(**opts_cr).relabel('Background Temporal Initial')\n",
    " + regrid(hv.Image(b_spatial, kdims=['width', 'height'])).opts(**opts_im).relabel('Background Spatial First Update')\n",
    " + datashade(hv.Curve(f_spatial, kdims=['frame'])).opts(**opts_cr).relabel('Background Temporal First Update')\n",
    ").cols(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test parameters for temporal update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First off we select some `units` to do parameter exploring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interactive:\n",
    "    units = np.random.choice(A_spatial.coords['unit_id'], 10, replace=False)\n",
    "    units.sort()\n",
    "    A_sub = A_spatial.sel(unit_id=units).persist()\n",
    "    C_sub = C_spatial.sel(unit_id=units).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we move on to the parameter exploring of temporal update. Here we use the same idea we have before, only this time there is much more parameters to play with for temporal update, and we now have four `list`s of potential parameters: `p_ls`, `sprs_ls`, `add_ls`, and `noise_ls`. We use [`itertools.product`](https://docs.python.org/3.7/library/itertools.html#itertools.product) to iterate through all possible combinations of the potential values and save us from nested `for` loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if interactive:\n",
    "    p_ls = [1]\n",
    "    sprs_ls = [0.01, 0.05, 0.1, 2]\n",
    "    add_ls = [20]\n",
    "    noise_ls = [0.06]\n",
    "    YA_dict, C_dict, S_dict, g_dict, sig_dict, A_dict = [dict() for _ in range(6)]\n",
    "    YrA = compute_trace(Y, A_sub, b_spatial, C_sub, f_spatial).persist()\n",
    "    for cur_p, cur_sprs, cur_add, cur_noise in itt.product(p_ls, sprs_ls, add_ls, noise_ls):\n",
    "        ks = (cur_p, cur_sprs, cur_add, cur_noise)\n",
    "        print(\"p:{}, sparse penalty:{}, additional lag:{}, noise frequency:{}\"\n",
    "              .format(cur_p, cur_sprs, cur_add, cur_noise))\n",
    "        YrA, cur_C, cur_S, cur_B, cur_C0, cur_sig, cur_g, cur_scal = update_temporal(\n",
    "            Y, A_sub, b_spatial, C_sub, f_spatial, sn_spatial, YrA=YrA,\n",
    "            sparse_penal=cur_sprs, p=cur_p, use_spatial=False, use_smooth=True,\n",
    "            add_lag = cur_add, noise_freq=cur_noise)\n",
    "        YA_dict[ks], C_dict[ks], S_dict[ks], g_dict[ks], sig_dict[ks], A_dict[ks] = (\n",
    "            YrA, cur_C, cur_S, cur_g, cur_sig, A_sub)\n",
    "    hv_res = visualize_temporal_update(\n",
    "        YA_dict, C_dict, S_dict, g_dict, sig_dict, A_dict,\n",
    "        kdims=['p', 'sparse penalty', 'additional lag', 'noise frequency'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A piece of useful infomation after you run this cell is that under what testing parameter, which sample units got dropped because of poor fit:\n",
    "![dropped sample units](img/first_tem_drop_v2.PNG)\n",
    "<div class=\"alert alert-success\">  \n",
    "Cross compare this with the raw trace plot, find the most reasonable parameters that drop the right sample cells.\n",
    "</div>\n",
    "\n",
    "Then,  we plot the visualization `hv_res` of the 10 ramdom units we just generated at the belowing code cell. Don't worry if each parameter doesn't make much sense now, What you should expect here will be explained later in <strong>first temporal update</strong> along with what `param_first_temporal` actually does (Look for the green tips box)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.output(size=output_size)\n",
    "if interactive:\n",
    "    display(hv_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## first temporal update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the idea for temporal update: Recall tha parameters:\n",
    "\n",
    "```python\n",
    "param_first_temporal = {\n",
    "    'noise_freq': 0.06,\n",
    "    'sparse_penal': 0.1,\n",
    "    'p': 1,\n",
    "    'add_lag': 20,\n",
    "    'use_spatial': False,\n",
    "    'jac_thres': 0.2,\n",
    "    'zero_thres': 1e-8,\n",
    "    'max_iters': 200,\n",
    "    'use_smooth': True,\n",
    "    'scs_fallback': False,\n",
    "    'post_scal': True}\n",
    "```\n",
    "\n",
    "Similar to the spatial update, given  the spatial footprint of each unit (`A`), our goal is now to find the activity of each unit (`C`) that minimizes both the **error** (`Y - A.dot(C, 'unit_id')`) and the **l1-norm** of `C`. However there is an additional constraint: the trace of each unit in `C` must follow an autoregressive process. Due to this additional layer of complexity, things becomes more computationaly expensive.  To reduce computatioinal cost, first observe that `A` is usually much larger than `C` (you usually have more total pixels than `frame`s), and performing the dot product, `A.dot(C, 'unit_id')`, everytime you try a different number in `C`,  is infeasible. Thus, we convert our **error** term to something like $\\mathbf{A}^{-1} \\cdot \\mathbf{Y} - \\mathbf{C}$, where $\\mathbf{A}^{-1}$ represents a matrix that can \"undo\" what `A` usually does to `C` -- instead of weighting the temporal activity of each unit by its spatial footprint (converting a matrix with dimension `unit_id` and `frame` into one with dimensions `height`, `width` and `frame`), $\\mathbf{A}^{-1}$ \"extracts\" the temporal activity of each unit based upon their spatial footprint (converting a matrix with dimension `height`, `width` and `frame` into one with dimensions `unit_id` and `frame`). In other words, $\\mathbf{A}^{-1}$ is like an [inverse](https://en.wikipedia.org/wiki/Moore–Penrose_inverse) of `A`. This way, we only need to calculate $\\mathbf{A}^{-1} \\cdot \\mathbf{Y}$ once and be done -- we can use that result everytime we update `C`. The calculation of $\\mathbf{A}^{-1} \\cdot \\mathbf{Y}$ is rather complicated and not strictly mathematically accurate, but it provides a good approximation with huge computational benefit, and is the default behavior of CaImAn. You can turn this off by supplying `use_spatial=True` -- however that is usually too computationally demanding to do. We will assume `use_spatial=False` in the following discussion and call the $\\mathbf{A}^{-1} \\cdot \\mathbf{Y}$ term `YrA`, as in the code. The second thing to observe is that we cannot keep the `unit_id` dimension and chop up the `frame` dimension for parallel processing (like how we chopped up pixels during the spatial update), since we have to check whether each trace along the `frame` dimension follows an autoregressive process. Instead, we turn to the `unit_id` dimension to make our problem \"smaller\". Since we have a relatively good `A` now, it should be OK to update units that are not spatially overlapping independently. This idea should work if you have a relatively sparse distribution of cells. However if your field-of-view is packed with cells, if we were to consider cells overlapping if they share only one pixel, we would likely end up having to update `C` altogether, since every cell is transitively overlapping with every other cell. Instead, we put a threshold on how we define \"overlap\", and that is what `jac_thres` is for -- only cells that have an area of their spatial footprint overlapping that is more than this threshold (ranging from 0 to 1) will be considered \"overlapping\". (The \"proportion of overlapping area\" has a formal name: [Jaccard index](https://en.wikipedia.org/wiki/Jaccard_index), hence the name `jac_thres`). Pragamatically `jac_thres=0.2` works for data that is very compact in cells.\n",
    "\n",
    "We now turn to the \"other layer of complexity,\" which is the autoregressive process. Recall that the temporal trace of each unit should be fitted by the following equation: $$c(t) = \\sum_{i=0}^{p}\\gamma_i c(t-i) + s(t) + \\epsilon$$ The first thing we want to determine is `p`. As discussed before, `p=2` is a good choice if your calcium transients have an observable rise-time. `p=1` might work better if the rise-time of your signal is faster than your sampling rate and you thus don't need to explicitly model it. Notably, `p>2` could result in [over-fitting](https://en.wikipedia.org/wiki/Overfitting) and is not recomended unless you are certain that your calcium traces have a more complicated waveform. Next, notice that we have several $\\gamma_i$s unaccounted for (though usually not too many if `p` is small). Luckily, we do not have to iteratively update these -- it turns out that the $\\gamma_i$s of an autoregressive process are related to the [autocovariance](https://en.wikipedia.org/wiki/Autocovariance) of the signal at different lags, which can be readily computed from `YrA`. For full derivation of these relationships, please refer to the [original CNMF paper](https://www.sciencedirect.com/science/article/pii/S0896627315010843?via%3Dihub). Here, we will merely assume that the parameters that affect how much a signal depends on its own history are related to the covariance of the signal when you shift it by different temporal lags. In this way, $\\gamma_i$s can be computed rather deterministicly. Say you set `p=2` and thus you have two $\\gamma_i$s to be estimated -- you would need exactly two equations involving the autocovariance function up to 2 time-step lags to give you the two $\\gamma_i$s. However, you can add additional equations using different lags to better model the propogation of signal, since the impact of $\\gamma_i$s can theoretically extend infinitely back in time, and should be reflected in the autocovariance function at any additional lag. In practice, we use a finite number of equations, solved with [least squares](https://en.wikipedia.org/wiki/Least_squares). Thus it is important to choose an appropriate number of **additional** equations, which is what `add_lag` controls. An `add_lag` that is too small like `add_lag=0` will leave everything to the first `p` number of equations and autocovariance functions, which might not be reliable. Pragmatically, smaller `add_lag` values tend to bias the $\\gamma_i$s to give a much faster decay, whereas larger `add_lag` values tend to give a longer decay. **As a rule of thumb, it is usually good to set `add_lag` to approximately the decay time of your signal (in frames).** \n",
    "\n",
    "Once we have estimated the $\\gamma_i$s, the calcium traces, $c(t)$, and spikes, $s(t)$, are essentially **one thing** -- given calcium traces and how they rise/decay in response to spikes, we can deduce where the spikes happen, and *vice versa*. We can express this determined relationship with a matrix $\\mathbf{G}$ where $s(t) = \\mathbf{G} \\cdot c(t)$. In other words, $\\mathbf{G}$ is the matrix that \"undoes\" what $\\gamma_i$s do to $s(t)$. With all these parameters sorted out, we finally come to the actual optimization problem:\n",
    "\n",
    "$$\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "& \\underset{C_{i}}{\\text{minimize}}\n",
    "& & \\left \\lVert \\mathbf{YrA}_{i} - \\mathbf{C}_{i} \\right \\rVert ^2 + \\alpha \\left \\lvert \\mathbf{G}_{i} \\cdot \\mathbf{C}_{i} \\right \\rvert \\\\\n",
    "& \\text{subject to}\n",
    "& & \\mathbf{C}_{i} \\geq 0, \\; \\mathbf{G}_{i} \\cdot \\mathbf{C}_{i} \\geq 0 \n",
    "\\end{aligned}\n",
    "\\end{equation*}$$\n",
    "\n",
    "Just as during the spatial update, we select some units ($i$), and update their calcium dynamics ($\\mathbf{C}_i$) based on the **error** and the **l1-norm** of the **spikes** ($\\mathbf{G}_i \\cdot \\mathbf{C}_i$). Again, it does not make sense to have negative calcium dynamics or spikes, so that is a constraint on the problem. Moreover, we need an $\\alpha$ to provide balance between fidelity and sparsity, which can be scaled up and down with `sparse_penal` (`sparse_penal=1` is equivalent to the default behavior of CaImAn). Furthermore, $\\alpha$ should depend on the expected level of noise. Note that we cannot use `sn_spatial` since that was the noise for each pixel, and we need the noise for each unit. The function `update_temporal` estimates the noise of each unit for you -- you just have to tell it the `noise_freq`uency. Like before, **0.5** is the highest you can go. With the default, `noise_freq=0.25`, the higher frequency half of the signal will be considered noise. In addition to affecting the estimation of noise power, `noise_freq` affects another smoothing process: when estimating $\\gamma_i$s, it is usually helpful to run a filter on the signal to get rid of high freqeuency noise, particularly when you don't have a large `add_lag`. The parameter, `noise_freq` is the cut-off frequency of the low-pass filter run on the temporal trace for each unit.  Additionally, you can set the value of `use_smooth` to control whether the filtering is done at all. Even with this careful design, however, it is sometimes hard to approach the true solution to the problem. When that happens, `update_temporal` will warn you by saying something like \"problem solved sub-optimally\". Usually, a few of these warnings is OK, but if you see this warning a lot it either means your parameters are unreasonable or you need more iterations to approach the real answer. You can use `max_iters` to control how many iterations to run for each small problem before the computer gives up and throws a warning. Furthermore, in some very, very rare cases, the default [ecos solver](https://www.cvxpy.org/tutorial/advanced/index.html#choosing-a-solver) (the algorithm that does all the heavy-lifting) can fail and throw a \"problem infeasible\" warning, and it's worth trying a different solver, namely [scs](https://www.cvxpy.org/tutorial/advanced/index.html#choosing-a-solver).  Be aware that scs produces results with very, very slow performance. The boolean parameter `scs_fallback` controls whether the scs attempt should be made before giving up. Importantly, both increasing `max_iters` and using `scs_fallback` will significantly increase the computation time and will not help at all if the parameters you provided are unreasonable to begin with, so try to use this only as a last resort.\n",
    "\n",
    "Finally, after the optimization is done, and just like [`update_spatial`](#first-spatial-update), we have a `zero_thres` to get rid of the small numbers, after which we can do a `post_scal` to counter the artifacts introduced by the **l1-norm** penalty.\n",
    "\n",
    "`update_temporal` takes in `Y`, `A`, `b`, `C`, `f`, and `sn_spatial` (even if we won't need it by default), in that order. Optionally you can pass in `noise_freq`, `p`, `add_lag`, `jac_thres`, `use_spatial`, `sparse_penal`, `max_iters`, `use_smooth`, `scs_fallback`, `zero_thres` and `post_scal`, as we have discussed. `update_temporal` returns much more than we expected -- in addition to `C_temporal` and `S_temporal`, which are the results we care most about, it also returns `YrA`, and `g_temporal` (the $\\mathbf{G}$ matrix for each unit). Moreover, it returns `B_temporal`, `C0_temporal` and `sig_temporal`, representing the final layer of complexity: when we update the temporal trace, there might be a global baseline calcium concentration, which is modeled by $b$ and returned in `B_temporal`. A spike may also have happened right before recording starts and the resulting calcium transient could still be decaying in the first few seconds, so we model this with an initial calcium concentration, $c_0$, that follows the same decaying pattern defined by $\\gamma_i$s, and is returned in `C0_temporal`. Both $b$ and $c_0$ are single numbers that get updated along with the calcium dynamics for each unit. Finally there is `sig_temporal` which is the combination of all the signals, that is: `C_temporal + C0_temporal + B_temporal`\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "You should now have an idea of what each parameter is doing in `update_temporal`, and be able to make sense of the visualization results of the parameter exploring steps.\n",
    "    \n",
    "\n",
    "- As was briefly mentioned before, minian's output of <strong>dropped sample units</strong> information and visualization of their <strong>raw traces</strong> is useful after the first temporal update. Since one of the main purposes of the <strong>first temporal update</strong> is to get rid of trash cells and cells with noisy signal, successful parameter selection is evidenced by dropped units with raw traces that look like noise (no clear bursts of activity). Alternatively, if cell-like activity is seen in the raw trace of a dropped unit, this may indicate that the selected parameters are too conservative. \n",
    "\n",
    "- When reading the temporal trace plot, \"fitted spikes\" (green), \"fitted signal\" (orange), and \"fitted calcium trace\" (blue), are all alligned to the \"raw signal\" based upon the model. Ideally, we want only one spike for each burst of signal, with \"fitted signal\" and \"fitted calcium trace\" decaying in a manner that follows the raw signal. Below is the temporal plot of an example unit using different <strong>sparse_panel</strong>:\n",
    "\n",
    "### Example Temporal Traces\n",
    "\n",
    "![example temporal traces](img/first_tem_param.png)\n",
    "\n",
    "\n",
    "\n",
    "Here, the top trace is when <strong>sparse_panel</strong> = 1, and we can see that there are lots of small spikes at the bottom, indicating we may want to increase the <strong>sparse_panel</strong> to get rid of them. However, when we are using <strong>sparse_panel</strong> = 10 (bottom panel), it's clear that we are missing real spikes from raw signal. Thus, the middle panel with <strong>sparse_panel</strong> = 3 fits the raw signal the best here.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below produces plots of temporal traces and spikes after the first temporal update and allows us to compare them to the signal originiating from the initialization step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "YrA, C_temporal, S_temporal, B_temporal, C0_temporal, sig_temporal, g_temporal, scale = update_temporal(\n",
    "    Y, A_spatial, b_spatial, C_spatial, f_spatial, sn_spatial, **param_first_temporal)\n",
    "A_temporal = A_spatial.sel(unit_id = C_temporal.coords['unit_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.output(size=output_size)\n",
    "opts_im = dict(frame_width=500, aspect=2, colorbar=True, cmap='Viridis', logz=True)\n",
    "(regrid(hv.Image(C_init.rename('ci'), kdims=['frame', 'unit_id'])).opts(**opts_im).relabel(\"Temporal Trace Initial\")\n",
    " + hv.Div('')\n",
    " + regrid(hv.Image(C_temporal.rename('c1'), kdims=['frame', 'unit_id'])).opts(**opts_im).relabel(\"Temporal Trace First Update\")\n",
    " + regrid(hv.Image(S_temporal.rename('s1'), kdims=['frame', 'unit_id'])).opts(**opts_im).relabel(\"Spikes First Update\")\n",
    ").cols(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell of code allows us to visualize units that were dropped during the first temporal update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.output(size=output_size)\n",
    "if interactive:\n",
    "    h, w = A_spatial.sizes['height'], A_spatial.sizes['width']\n",
    "    im_opts = dict(aspect=w/h, frame_width=500, cmap='Viridis')\n",
    "    cr_opts = dict(aspect=3, frame_width=1000)\n",
    "    bad_units = list(set(A_spatial.coords['unit_id'].values) - set(A_temporal.coords['unit_id'].values))\n",
    "    bad_units.sort()\n",
    "    if len(bad_units)>0:\n",
    "        hv_res = (hv.NdLayout({\n",
    "            \"Spatial Footprin\": regrid(hv.Dataset(A_spatial.sel(unit_id=bad_units).rename('A'))\n",
    "                                       .to(hv.Image, kdims=['width', 'height'])).opts(**im_opts),\n",
    "            \"Spatial Footprints of Accepted Units\": regrid(hv.Image(A_temporal.sum('unit_id').compute().rename('A'), kdims=['width', 'height'])).opts(**im_opts)\n",
    "        })\n",
    "                  + datashade(hv.Dataset(YrA.sel(unit_id=bad_units).rename('raw'))\n",
    "                              .to(hv.Curve, kdims=['frame'])).opts(**cr_opts).relabel(\"Temporal Trace\")).cols(1)\n",
    "        display(hv_res)\n",
    "    else:\n",
    "        print(\"No rejected units to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can visualize the activity of each unit. There are four traces in the top plot: \"Raw Signal\" corresponds to `YrA`, \"Fitted Spikes\" to `S_temporal`, \"Fitted Calcium Trace\" to `C_temporal` and \"Fitted Signal\" to `sig_temporal`. The latter two traces usually overlap with each other since `B_temporal` and `C0_temporal` are often equal **0**. Sadly, due to large number of frames and the limitation of our browser, it is usually only possible to visualize 50 units at a time, hence `select(unit_id=slice(0, 50))`. Nevertheless it gives us an idea of how things went. Put in other numbers if you want to see other units. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.output(size=output_size)\n",
    "if interactive:\n",
    "    display(visualize_temporal_update(YrA, C_temporal, S_temporal, g_temporal, sig_temporal, A_temporal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing CNMF cannot do is merge together units that belong to the same cell. Even though we tried something similar during [initialization](#initialization), we might miss some, and it is better to do it here again. Recall the parameters:\n",
    "\n",
    "```python\n",
    "param_first_merge = {\n",
    "    'thres_corr': 0.9}\n",
    "```\n",
    "\n",
    "The idea is straight-forward and based purely on pearson correlation of temporal activities. Any units whose spatial footprints share at least one pixel are considered potential targets for merging, and any of these units that have a pearson correlation of temporal activities higher than `thres_corr` will be merged. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "A_mrg, sig_mrg, add_list = unit_merge(A_temporal, sig_temporal, [S_temporal, C_temporal], **param_first_merge)\n",
    "S_mrg, C_mrg = add_list[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can visualize the results of unit merging. The left panel shows the original temporal signal, while the right panel shows the temporal signal after merging.\n",
    "<div class=\"alert alert-info\">\n",
    "Ideally, you want to see units in the left panel with <strong>too</strong> similar of signals, merged in the right penal. Adjust the <strong>thres_corr</strong> in <strong>param_first_merge</strong> accordingly.\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.output(size=output_size)\n",
    "opts_im = dict(frame_width=500, aspect=2, colorbar=True, cmap='Viridis', logz=True)\n",
    "(regrid(hv.Image(sig_temporal.rename('c1'), kdims=['frame', 'unit_id'])).relabel(\"Temporal Signals Before Merge\").opts(**opts_im) +\n",
    "regrid(hv.Image(sig_mrg.rename('c2'), kdims=['frame', 'unit_id'])).relabel(\"Temporal Signals After Merge\").opts(**opts_im))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test parameters for spatial update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is almost identical to the [first time](#test-parameters-for-first-spatial-update) we explore spatial parameters, except for changes in variable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interactive:\n",
    "    units = np.random.choice(A_mrg.coords['unit_id'], 10, replace=False)\n",
    "    units.sort()\n",
    "    A_sub = A_mrg.sel(unit_id=units).persist()\n",
    "    sig_sub = sig_mrg.sel(unit_id=units).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">  \n",
    "Again, you can simply <strong>add</strong> the values that you want to test to <strong>sprs_ls</strong>. Pragmatically, it's generally fine to use the same <strong>sprs_ls</strong> from the first spatial update or one that is a little smaller.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if interactive:\n",
    "    sprs_ls = [0.001, 0.005, 0.01]\n",
    "    A_dict = dict()\n",
    "    C_dict = dict()\n",
    "    for cur_sprs in sprs_ls:\n",
    "        cur_A, cur_b, cur_C, cur_f = update_spatial(\n",
    "            Y, A_sub, b_init, sig_sub, f_init,\n",
    "            sn_spatial, dl_wnd=param_second_spatial['dl_wnd'], sparse_penal=cur_sprs)\n",
    "        if cur_A.sizes['unit_id']:\n",
    "            A_dict[cur_sprs] = cur_A\n",
    "            C_dict[cur_sprs] = cur_C\n",
    "    hv_res = visualize_spatial_update(A_dict, C_dict, kdims=['sparse penalty'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.output(size=output_size)\n",
    "if interactive:\n",
    "    display(hv_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "Again, use the visualization results here to help choose the <strong>sparse_panel</strong> and <strong>dl_wnd</strong>, to use in the next step.  Be sure to update the paramaters.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## second spatial update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the second iteration of the spatial update. It is identical to [first spatial update](#first-spatial-update), with the exception of appending **it2**s after the variable names, standing for \"iteration 2\". From this, it should be apparentt that if you you can modify the code to have more cycles of spatial updates followed by temporal updates. Simply add more sections like this and [the section below](#second-temporal-update)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "A_spatial_it2, b_spatial_it2, C_spatial_it2, f_spatial_it2 = update_spatial(\n",
    "    Y, A_mrg, b_spatial, sig_mrg, f_spatial, sn_spatial, **param_second_spatial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.output(size=output_size)\n",
    "opts = dict(aspect=A_spatial_it2.sizes['width']/A_spatial_it2.sizes['height'], frame_width=500, colorbar=True, cmap='Viridis')\n",
    "(regrid(hv.Image(A_mrg.sum('unit_id').compute().rename('A'), kdims=['width', 'height'])).opts(**opts).relabel(\"Spatial Footprints First Update\")\n",
    "+ regrid(hv.Image((A_mrg.fillna(0) > 0).sum('unit_id').compute().rename('A'), kdims=['width', 'height']), aggregator='max').opts(**opts).relabel(\"Binary Spatial Footprints First Update\")\n",
    "+ regrid(hv.Image(A_spatial_it2.sum('unit_id').compute().rename('A'), kdims=['width', 'height'])).opts(**opts).relabel(\"Spatial Footprints Second Update\")\n",
    "+ regrid(hv.Image((A_spatial_it2 > 0).sum('unit_id').compute().rename('A'), kdims=['width', 'height']), aggregator='max').opts(**opts).relabel(\"Binary Spatial Footprints Second Update\")).cols(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here again, visualize the result of second spatial update, if not satisfying with this, feel free to reset **param_second_spatial** and rerun this session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.output(size=output_size)\n",
    "opts_im = dict(aspect=b_spatial_it2.sizes['width'] / b_spatial_it2.sizes['height'], frame_width=500, colorbar=True, cmap='Viridis')\n",
    "opts_cr = dict(aspect=2, frame_height=int(500 * b_spatial_it2.sizes['height'] / b_spatial_it2.sizes['width']))\n",
    "(regrid(hv.Image(b_spatial, kdims=['width', 'height'])).opts(**opts_im).relabel('Background Spatial First Update')\n",
    " + datashade(hv.Curve(f_spatial, kdims=['frame'])).opts(**opts_cr).relabel('Background Temporal First Update')\n",
    " + regrid(hv.Image(b_spatial_it2, kdims=['width', 'height'])).opts(**opts_im).relabel('Background Spatial Second Update')\n",
    " + datashade(hv.Curve(f_spatial_it2, kdims=['frame'])).opts(**opts_cr).relabel('Background Temporal Second Update')\n",
    ").cols(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test parameters for temporal update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is almost identical to the [first time](#test-parameters-for-first-temporal-update) except for variable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interactive:\n",
    "    units = np.random.choice(A_spatial_it2.coords['unit_id'], 10, replace=False)\n",
    "    units.sort()\n",
    "    A_sub = A_spatial_it2.sel(unit_id=units).persist()\n",
    "    C_sub = C_spatial_it2.sel(unit_id=units).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "Generally, our aim here for the second temporal update is too refine the model and make the \"fitted spikes\", \"fitted signal\", and \"fitted calcium trace\" fit the \"raw signal\" better.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if interactive:\n",
    "    p_ls = [1]\n",
    "    sprs_ls = [0.01, 0.05, 0.1]\n",
    "    add_ls = [20]\n",
    "    noise_ls = [0.06]\n",
    "    YA_dict, C_dict, S_dict, g_dict, sig_dict, A_dict = [dict() for _ in range(6)]\n",
    "    YrA = compute_trace(Y, A_sub, b_spatial, C_sub, f_spatial).persist()\n",
    "    for cur_p, cur_sprs, cur_add, cur_noise in itt.product(p_ls, sprs_ls, add_ls, noise_ls):\n",
    "        ks = (cur_p, cur_sprs, cur_add, cur_noise)\n",
    "        print(\"p:{}, sparse penalty:{}, additional lag:{}, noise frequency:{}\"\n",
    "              .format(cur_p, cur_sprs, cur_add, cur_noise))\n",
    "        YrA, cur_C, cur_S, cur_B, cur_C0, cur_sig, cur_g, cur_scal = update_temporal(\n",
    "            Y, A_sub, b_spatial, C_sub, f_spatial, sn_spatial, YrA=YrA,\n",
    "            sparse_penal=cur_sprs, p=cur_p, use_spatial=False, use_smooth=True,\n",
    "            add_lag = cur_add, noise_freq=cur_noise)\n",
    "        YA_dict[ks], C_dict[ks], S_dict[ks], g_dict[ks], sig_dict[ks], A_dict[ks] = (\n",
    "            YrA, cur_C, cur_S, cur_g, cur_sig, A_sub)\n",
    "    hv_res = visualize_temporal_update(\n",
    "        YA_dict, C_dict, S_dict, g_dict, sig_dict, A_dict,\n",
    "        kdims=['p', 'sparse penalty', 'additional lag', 'noise frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.output(size=output_size)\n",
    "if interactive:\n",
    "    display(hv_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## second temporal update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is identical to the [first temporal update](#first-temporal-update) except for variable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "YrA, C_temporal_it2, S_temporal_it2, B_temporal_it2, C0_temporal_it2, sig_temporal_it2, g_temporal_it2, scale_temporal_it2 = update_temporal(\n",
    "    Y, A_spatial_it2, b_spatial_it2, C_spatial_it2, f_spatial_it2, sn_spatial, **param_second_temporal)\n",
    "A_temporal_it2 = A_spatial_it2.sel(unit_id=C_temporal_it2.coords['unit_id'])\n",
    "g_temporal_it2 = g_temporal_it2.sel(unit_id=C_temporal_it2.coords['unit_id'])\n",
    "A_temporal_it2 = rechunk_like(A_temporal_it2, A_spatial_it2)\n",
    "g_temporal_it2 = rechunk_like(g_temporal_it2, C_temporal_it2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.output(size=output_size)\n",
    "opts_im = dict(frame_width=500, aspect=2, colorbar=True, cmap='Viridis', logz=True)\n",
    "(regrid(hv.Image(C_mrg.rename('c1'), kdims=['frame', 'unit_id'])).opts(**opts_im).relabel(\"Temporal Trace First Update\")\n",
    " + regrid(hv.Image(S_mrg.rename('s1'), kdims=['frame', 'unit_id'])).opts(**opts_im).relabel(\"Spikes First Update\")\n",
    " + regrid(hv.Image(C_temporal_it2.rename('c2').rename(unit_id='unit_id_it2'), kdims=['frame', 'unit_id_it2'])).opts(**opts_im).relabel(\"Temporal Trace Second Update\")\n",
    " + regrid(hv.Image(S_temporal_it2.rename('s2').rename(unit_id='unit_id_it2'), kdims=['frame', 'unit_id_it2'])).opts(**opts_im).relabel(\"Spikes Second Update\")).cols(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we visualize all the units that are dropped during this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.output(size=output_size)\n",
    "if interactive:\n",
    "    h, w = A_spatial_it2.sizes['height'], A_spatial_it2.sizes['width']\n",
    "    im_opts = dict(aspect=w/h, frame_width=500, cmap='Viridis')\n",
    "    cr_opts = dict(aspect=3, frame_width=1000)\n",
    "    bad_units = list(set(A_spatial_it2.coords['unit_id'].values) - set(A_temporal_it2.coords['unit_id'].values))\n",
    "    bad_units.sort()\n",
    "    if len(bad_units)>0:\n",
    "        hv_res = (hv.NdLayout({\n",
    "            \"Spatial Footprin\": regrid(hv.Dataset(A_spatial_it2.sel(unit_id=bad_units).rename('A'))\n",
    "                                       .to(hv.Image, kdims=['width', 'height'])).opts(**im_opts),\n",
    "            \"Spatial Footprints of Accepted Units\": regrid(hv.Image(A_temporal_it2.sum('unit_id').compute().rename('A'), kdims=['width', 'height'])).opts(**im_opts)\n",
    "        })\n",
    "                  + datashade(hv.Dataset(YrA.sel(unit_id=bad_units).rename('raw'))\n",
    "                              .to(hv.Curve, kdims=['frame'])).opts(**cr_opts).relabel(\"Temporal Trace\")).cols(1)\n",
    "        display(hv_res)\n",
    "    else:\n",
    "        print(\"No rejected units to display\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.output(size=output_size)\n",
    "if interactive:\n",
    "    display(visualize_temporal_update(YrA, C_temporal_it2, S_temporal_it2, g_temporal_it2, sig_temporal_it2, A_temporal_it2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we save our results in the `minian` dataset. Note that you can save any other variables by calling `save_minian` and using the code below as a reference. For example, you might want to consider using `sig_temporal` instead of `C_temporal` for your subsequent analysis. Also, you are not restricted to use the [netcdf](https://www.unidata.ucar.edu/software/netcdf/) format, though it is recommended. [Explore the xarray documentation](http://xarray.pydata.org/en/stable/io.html) for all IO options, and moreover, [numpy IO capabilities](https://docs.scipy.org/doc/numpy-1.15.0/reference/routines.io.html), since `xarray` is built on top of `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "A_temporal_it2 = save_minian(A_temporal_it2.rename('A'), **param_save_minian)\n",
    "C_temporal_it2 = save_minian(C_temporal_it2.rename('C'), **param_save_minian)\n",
    "S_temporal_it2 = save_minian(S_temporal_it2.rename('S'), **param_save_minian)\n",
    "g_temporal_it2 = save_minian(g_temporal_it2.rename('g'), **param_save_minian)\n",
    "C0_temporal_it2 = save_minian(C0_temporal_it2.rename('C0'), **param_save_minian)\n",
    "B_temporal_it2 = save_minian(B_temporal_it2.rename('B'), **param_save_minian)\n",
    "b_spatial_it2 = save_minian(b_spatial_it2.rename('b'), **param_save_minian)\n",
    "f_spatial_it2 = save_minian(f_spatial_it2.rename('f'), **param_save_minian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load the data we just saved for visualization purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minian = open_minian(dpath,\n",
    "                     fname=param_save_minian['fname'],\n",
    "                     backend=param_save_minian['backend'])\n",
    "varr = load_videos(dpath, **param_load_videos)\n",
    "chk = get_optimal_chk(varr.astype(float), dim_grp=[('frame',), ('height', 'width')])\n",
    "varr = varr.chunk(dict(frame=chk['frame']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell calls `generate_videos` to create a video that can help us quickly visualize the results. Under default settings, this video will be saved in your data folder. `generate_videos` takes in the dataset that contains cnmf results, an array representation of the raw video, the full path to the output video file, and a `dict` specifying chunks for performance. The resulting video will have four parts - Top left is the **Raw Video** after pre-processing and motion correction `minian['org']`; Top right is the **Processed Video** `minian['Y']` (that is, after pre-processing and motion correction); Bottom left is the **Residule**, that is **Raw Video** - **Units**. Bottom right is the **Units** from CNMF `minian['A'].dot(minian['C'], 'unit_id')`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "generate_videos(\n",
    "    minian, varr, dpath, param_save_minian['fname'] + \".mp4\", scale='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have a `CNMFViewer` to visualize the final results. \n",
    "<div class=\"alert alert-info\">\n",
    "<strong>Top Left panel</strong>-- spatial footprints of all cells (a sum projection).   \n",
    "    \n",
    "<strong>Top Middle panel</strong> `if UseAC` -- the dot product of A (spatial footprint) and C (temporal activities) matrix of selected neurons.   \n",
    "                              `if not UseAC` -- the spatial footprints of selected neurons (a sum projection).\n",
    "             \n",
    "\n",
    "<strong>Top Right panel</strong>-- raw video after pre-processing and motion correction, which is the movie that's fed in as `org` to `CNMFViewer`, if nothing is fed in it's `minian['org']`.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    " \n",
    "The <strong>Bottom Left Controller Panel</strong> has several useful features:\n",
    "\n",
    "<strong>Refresh</strong> -- refreshes the data when you switch to a new group of units and it is not loading properly.\n",
    "\n",
    "\n",
    "<strong>Load Data</strong> -- loads the data into memory, which will take some time by itself, but will make the later visualization faster.\n",
    "\n",
    "\n",
    "<strong>UseAC</strong> check box -- choose whether or not you want the middle panel to be the dot product of A (spatial footprint) and C (temporal activities) matrix of selected neurons. Note that this will make visualization process slower.\n",
    "\n",
    "\n",
    "<strong>Normalize</strong> -- normalizes the bottom middle trace and spike plot for each unit to itself.\n",
    "\n",
    "\n",
    "<strong>ShowC</strong> -- shows calcium traces for each unit across time in the bottom middle plot.\n",
    "\n",
    "\n",
    "<strong>ShowS</strong> -- shows spikes for each unit across time in the bottom middle plot.\n",
    "\n",
    "\n",
    "<strong>Previous Group</strong> and <strong>Next Group</strong> buttons -- allow you to easily go backward/forward to another group of units.\n",
    "\n",
    "\n",
    "<strong>Video Play Panel</strong> -- lets you play the top middle and right panel in real time.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "The <strong>Bottom Middle Panel</strong> contains plots of units along the time axis. Each group will have 4-5 units showing in the plot. Combine the plot with the videos to check the quality of your CNMF results. \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "The <strong>Bottom Right panel</strong>. is a labeling tool for you to manually kick out \"bad\" units by labelling them (they will be demarcated with a `-1`). You can also flag units to be merged if their temporal activities and spatial footprint suggest they should be.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if interactive:\n",
    "    cnmfviewer = CNMFViewer(minian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.output(size=output_size)\n",
    "if interactive:\n",
    "    display(cnmfviewer.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cell serves to save your manually changed labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interactive:\n",
    "    save_minian(cnmfviewer.unit_labels, **param_save_minian)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "name": "pipeline.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
